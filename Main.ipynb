{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39995d0e-17ac-40e0-a658-776ac701ab15",
   "metadata": {},
   "source": [
    "## Question 1: OR Gate Neural Network\n",
    "\n",
    "Calculate the output of the network (via forward propagation) given an input of $[1, 0]$.\n",
    "\n",
    "Answer:\n",
    "\n",
    "We have:  \n",
    "- Input: $\\mathbf{x} = [1, 0]$  \n",
    "- Hidden Layer Weights ($\\mathbf{H_1}$):\n",
    "\n",
    "  $\n",
    "  H_1 =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0 & 1 & 0 \\\\\n",
    "  0 & 1 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "  $\n",
    "- Hidden Layer Bias ($\\mathbf{B_1}$):\n",
    "\n",
    "  $\n",
    "  B_1 =\n",
    "  \\begin{bmatrix} 0.15 & 0.15 & 0.15 & 0.15 \\end{bmatrix}\n",
    "  $\n",
    "- Output Layer Weights ($\\mathbf{H_2}$):\n",
    "\n",
    "  $\n",
    "  H_2 =\n",
    "  \\begin{bmatrix} 1 & 1 & 0 & 1.5 \\end{bmatrix}\n",
    "  $\n",
    "- Output Layer Bias ($\\mathbf{B_2}$):\n",
    "\n",
    "  $\n",
    "  B_2 = 0\n",
    "  $\n",
    "\n",
    "As the hidden layer activation function is tanh, and the hidden layer output is computed as:\n",
    "\n",
    "$\n",
    "\\mathbf{A1} = H_1 \\cdot \\mathbf{x} + B_1\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathbf{A1} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.15 & 0.15 & 0.15 & 0.15 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix} 1 + 0 + 0.15 & 0 + 0 + 0.15 & 1 + 0 + 0.15 & 0 + 0 + 0.15 \\end{bmatrix} = \\begin{bmatrix} 1.15 & 0.15 & 1.15 & 0.15 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Applying the tanh activation function:\n",
    "\n",
    "$\n",
    "h = \\tanh(A1) = \\begin{bmatrix} \\tanh(1.15) & \\tanh(0.15) & \\tanh(1.15) & \\tanh(0.15) \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$\n",
    "\\tanh(1.15) \\approx 0.818, \\quad \\tanh(0.15) \\approx 0.149\n",
    "$\n",
    "\n",
    "$\n",
    "h = \\begin{bmatrix} 0.818 & 0.149 & 0.818 & 0.149 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Now, computing the output layer:\n",
    "\n",
    "$\n",
    "A2 = H_2 \\cdot h + B_2\n",
    "$\n",
    "\n",
    "$\n",
    "A2 =\n",
    "\\begin{bmatrix} 1 & 1 & 0 & 1.5 \\end{bmatrix}\n",
    "\\begin{bmatrix} 0.818 & 0.149 & 0.818 & 0.149 \\end{bmatrix} + 0\n",
    "$\n",
    "\n",
    "$\n",
    "= (1 \\times 0.818) + (1 \\times 0.149) + (0 \\times 0.818) + (1.5 \\times 0.149)\n",
    "$\n",
    "\n",
    "$\n",
    "= 0.818 + 0.149 + 0 + 0.223 = 1.19\n",
    "$\n",
    "\n",
    "Applying tanh activation:\n",
    "\n",
    "$\n",
    "\\tanh(1.19) \\approx 0.831\n",
    "$\n",
    "\n",
    "So, the final output is:\n",
    "\n",
    "$\n",
    "\\mathbf{A2} = 0.831\n",
    "$\n",
    "\n",
    "So, now updating weights using Gradient Descent:\n",
    "We are given, Target Output: $ t = 1 $  and Error (MSE Loss Derivative):\n",
    "\n",
    "$\n",
    "E = (y - t)^2\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{dE}{dy} = 2 (y - t) = 2 (0.831 - 1) = -0.338\n",
    "$\n",
    "\n",
    "However, since $ y $ has a tanh activation, we apply the chain rule:\n",
    "\n",
    "$\n",
    "\\frac{dy}{dA2} = 1 - \\tanh^2(A2)\n",
    "$\n",
    "\n",
    "Since:\n",
    "\n",
    "$\n",
    "\\tanh(1.191) \\approx 0.831\n",
    "$\n",
    "\n",
    "$\n",
    "1 - \\tanh^2(1.191) = 1 - (0.831)^2 = 1 - 0.691 = 0.309\n",
    "$\n",
    "\n",
    "Now, applying the chain rule:\n",
    "\n",
    "$\n",
    "\\frac{dE}{dA2} = \\frac{dE}{dy} \\cdot \\frac{dy}{dA2}\n",
    "$\n",
    "\n",
    "$\n",
    "= (-0.338) \\times (0.309) = -0.104\n",
    "$\n",
    "\n",
    "\n",
    "Update Rules (Gradient Descent with Learning Rate = 10):\n",
    "\n",
    "$\n",
    "\\frac{dE}{dH_2} = \\frac{dE}{dA2} \\cdot h\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{dE}{dB_2} = \\frac{dE}{dA2}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{dE}{dH_2} = (-0.104) \\times\n",
    "\\begin{bmatrix} 0.818 & 0.149 & 0.818 & 0.149 \\end{bmatrix} =\n",
    "\\begin{bmatrix} -0.085 & -0.015 & -0.085 & -0.015 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{dE}{dB_2} = -0.104\n",
    "$\n",
    "\n",
    "Using the learning rate $ \\eta = 10 $:\n",
    "\n",
    "$\n",
    "H_2' = H_2 - \\eta \\times \\frac{dE}{dH_2}\n",
    "$\n",
    "\n",
    "$\n",
    "B_2' = B_2 - \\eta \\times \\frac{dE}{dB_2}\n",
    "$\n",
    "\n",
    "Updating $ H_2 $:\n",
    "\n",
    "$\n",
    "H_2' =\n",
    "\\begin{bmatrix} 1 & 1 & 0 & 1.5 \\end{bmatrix} -\n",
    "10 \\times\n",
    "\\begin{bmatrix} -0.085 & -0.015 & -0.085 & -0.015 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix} 1 & 1 & 0 & 1.5 \\end{bmatrix} + \\begin{bmatrix} 0.85 & 0.15 & 0.85 & 0.15 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "= \\begin{bmatrix} 1.85 & 1.15 & 0.85 & 1.65 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Updating $ B_2 $:\n",
    "\n",
    "$\n",
    "B_2' = 0 - 10 \\times (-0.104) = 1.04\n",
    "$\n",
    "\n",
    "Now, Final Updated Values are\n",
    "$\n",
    "H_2 = \\begin{bmatrix} 1.85 & 1.15 & 0.85 & 1.65 \\end{bmatrix}, \\quad B_2 = 1.04\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48769d7-f9c0-48e6-b98a-d37ccadc5c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Propagation Output (y): 0.8305685288556741\n",
      "Gradient of H2: [-0.08594624 -0.01564787 -0.08594624 -0.01564787]\n",
      "Gradient of B2: -0.10510034723806064\n",
      "Updated H2: [1.85946238 1.15647869 0.85946238 1.65647869]\n",
      "Updated B2: 1.0510034723806063\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Weights and Bias for both layers (Hidden and Output)\n",
    "H1 = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
    "B1 = np.array([0.15, 0.15, 0.15, 0.15])\n",
    "H2 = np.array([1, 1, 0, 1.5])\n",
    "B2 = 0\n",
    "\n",
    "# Here x is the given input data, target is the required output and lr is the learning rate\n",
    "x = np.array([1, 0])\n",
    "target = 1\n",
    "lr = 10\n",
    "\n",
    "# Implementation of both tanh and tanh Derivative is below\n",
    "def tanh(x):\n",
    "    z = np.tanh(x)\n",
    "    return z\n",
    "\n",
    "def tanDer(x):\n",
    "    zprime = 1 - np.tanh(x) ** 2\n",
    "    return zprime\n",
    "\n",
    "# Now I will be calculating the output of hidden layer and the output layer - y is the output predicted\n",
    "A1 = np.dot(x, H1) + B1\n",
    "h = tanh(A1)\n",
    "\n",
    "A2 = np.dot(H2, h) + B2\n",
    "y = tanh(A2)\n",
    "\n",
    "# Now I will compute the gradients and use the chain rule and backpropagate\n",
    "# The below is the gradient of error function (MSE)\n",
    "dEdy = 2 * (y - target)\n",
    "dydA2 = tanDer(A2)\n",
    "\n",
    "# Now finding the gradient for the input of the output layer using the output error of the output layer\n",
    "dEdA2 = dEdy * dydA2\n",
    "\n",
    "# Now finding gradient with respect to the weights and bias\n",
    "dEdH2 = dEdA2 * h\n",
    "dEdB2 = dEdA2\n",
    "\n",
    "# Finally updating H2 and B2 (Weights and Bias of the output layer)\n",
    "H2new = H2 - lr * dEdH2\n",
    "B2new = B2 - lr * dEdB2\n",
    "\n",
    "print(\"Forward Propagation Output (y):\", y)\n",
    "print(\"Gradient of H2:\", dEdH2)\n",
    "print(\"Gradient of B2:\", dEdB2)\n",
    "print(\"Updated H2:\", H2new)\n",
    "print(\"Updated B2:\", B2new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49370ed2-635b-42b2-91a5-f0140ffb31af",
   "metadata": {},
   "source": [
    "### Question 2: Coding and Testing a Neural Network\n",
    "\n",
    "I implemented the Layer, FCLayer, ActivationLayer, and Network classes following the approach outlined in the question. The weights and bias can either be explicitly set as demonstrated in the test functions or Xavier initialized to prevent exploding or vanishing gradients.\n",
    "\n",
    "Apart from the basic classes, I included two more classes to select an appropriate loss function based on the output classes automatically. However, the users can specify a custom loss function manually, as seen in the testingNN and testingStress functions.\n",
    "\n",
    "The code is also reusable in the sense that users can construct any simple neural network architecture with various activation and loss functions provided that they are incorporated within the code. Modular functions are applied in the sense that they can be tested independently, and values can be manually set in order to test them in a controlled environment. For ease of reading and maintenance, I included a lot of comments within the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc559a1-8340-4f94-8bc6-10a0025d339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output:\n",
      "[[2 1 1]]\n",
      "Updated Weights after Backpropagation:\n",
      "[[ 9.980e-01  1.000e-03  9.995e-01]\n",
      " [-2.000e-03  1.001e+00 -5.000e-04]]\n",
      "Updated Biases after Backpropagation:\n",
      "[[ 9.98e-01  1.00e-03 -5.00e-04]]\n",
      "Sigmoid Activation Forward Output:\n",
      "[[0.62245933 0.37754067]\n",
      " [0.81757448 0.92414182]]\n",
      "[[ 0.01175019 -0.00470007]\n",
      " [ 0.01491465  0.00350519]]\n",
      "Backpropagation completed. Gradients applied.\n",
      "Training on XOR Function:\n",
      "Epoch 0, Train Loss: 0.6954743536405679, Validation Loss: 0.6849436720607741\n",
      "Epoch 100, Train Loss: 0.16845850016646624, Validation Loss: 0.18296687398209155\n",
      "Epoch 200, Train Loss: 0.05170702020425154, Validation Loss: 0.06059977097345672\n",
      "Epoch 300, Train Loss: 0.028944688433722263, Validation Loss: 0.03481612391059583\n",
      "Epoch 400, Train Loss: 0.019893936184996686, Validation Loss: 0.02425763300686692\n",
      "Epoch 500, Train Loss: 0.01510348960334612, Validation Loss: 0.018576724302997536\n",
      "Epoch 600, Train Loss: 0.01215421594256923, Validation Loss: 0.015041151112618889\n",
      "Epoch 700, Train Loss: 0.010160795030888603, Validation Loss: 0.012632627341587849\n",
      "Epoch 800, Train Loss: 0.00872531439603727, Validation Loss: 0.010887739902234435\n",
      "Epoch 900, Train Loss: 0.00764317443228633, Validation Loss: 0.009565991095772709\n",
      "Epoch 1000, Train Loss: 0.006798652701576014, Validation Loss: 0.008530362253178875\n",
      "Epoch 1100, Train Loss: 0.006121464798672843, Validation Loss: 0.007697141506495267\n",
      "Epoch 1200, Train Loss: 0.005566496374800545, Validation Loss: 0.007012329916748492\n",
      "Epoch 1300, Train Loss: 0.005103480445369345, Validation Loss: 0.006439547238147163\n",
      "Epoch 1400, Train Loss: 0.00471136323934292, Validation Loss: 0.005953394973309038\n",
      "Epoch 1500, Train Loss: 0.004375051552235746, Validation Loss: 0.005535606936769108\n",
      "Epoch 1600, Train Loss: 0.004083445629295546, Validation Loss: 0.005172711758124283\n",
      "Epoch 1700, Train Loss: 0.0038282020059497972, Validation Loss: 0.004854557515178799\n",
      "Epoch 1800, Train Loss: 0.0036029289710642013, Validation Loss: 0.004573349224524942\n",
      "Epoch 1900, Train Loss: 0.00340264802177652, Validation Loss: 0.00432300270170781\n",
      "Epoch 2000, Train Loss: 0.003223424151496275, Validation Loss: 0.004098699831520013\n",
      "Epoch 2100, Train Loss: 0.003062106333140616, Validation Loss: 0.0038965756527112715\n",
      "Epoch 2200, Train Loss: 0.002916141719831093, Validation Loss: 0.0037134938411677865\n",
      "Epoch 2300, Train Loss: 0.0027834402540188588, Validation Loss: 0.0035468827779206145\n",
      "Epoch 2400, Train Loss: 0.0026622744301752, Validation Loss: 0.003394613956115223\n",
      "Epoch 2500, Train Loss: 0.0025512040106347904, Validation Loss: 0.0032549104999828153\n",
      "Epoch 2600, Train Loss: 0.0024490187404114278, Validation Loss: 0.003126277443187319\n",
      "Epoch 2700, Train Loss: 0.002354694235794254, Validation Loss: 0.003007447960149113\n",
      "Epoch 2800, Train Loss: 0.002267357644625146, Validation Loss: 0.0028973414492276446\n",
      "Epoch 2900, Train Loss: 0.002186260644056564, Validation Loss: 0.0027950305285915898\n",
      "Epoch 3000, Train Loss: 0.0021107580104699334, Validation Loss: 0.002699714809939611\n",
      "Epoch 3100, Train Loss: 0.0020402904652845836, Validation Loss: 0.002610699880187088\n",
      "Epoch 3200, Train Loss: 0.0019743708337963024, Validation Loss: 0.0025273803234069333\n",
      "Epoch 3300, Train Loss: 0.001912572794156408, Validation Loss: 0.0024492259052017076\n",
      "Epoch 3400, Train Loss: 0.0018545216683479632, Validation Loss: 0.0023757702530534303\n",
      "Epoch 3500, Train Loss: 0.0017998868356500167, Validation Loss: 0.002306601522001382\n",
      "Epoch 3600, Train Loss: 0.001748375444732794, Validation Loss: 0.0022413546509880557\n",
      "Epoch 3700, Train Loss: 0.0016997271723322833, Validation Loss: 0.0021797049023866803\n",
      "Epoch 3800, Train Loss: 0.0016537098308336984, Validation Loss: 0.0021213624433257822\n",
      "Epoch 3900, Train Loss: 0.0016101156686275813, Validation Loss: 0.0020660677779524785\n",
      "Epoch 4000, Train Loss: 0.001568758239071703, Validation Loss: 0.002013587878718796\n",
      "Epoch 4100, Train Loss: 0.0015294697386829797, Validation Loss: 0.0019637128949939004\n",
      "Epoch 4200, Train Loss: 0.0014920987345425443, Validation Loss: 0.0019162533409326516\n",
      "Epoch 4300, Train Loss: 0.0014565082161183329, Validation Loss: 0.0018710376831141513\n",
      "Epoch 4400, Train Loss: 0.0014225739187464013, Validation Loss: 0.0018279102631844491\n",
      "Epoch 4500, Train Loss: 0.0013901828755920405, Validation Loss: 0.0017867295024541617\n",
      "Epoch 4600, Train Loss: 0.0013592321625812373, Validation Loss: 0.0017473663447916544\n",
      "Epoch 4700, Train Loss: 0.0013296278069600278, Validation Loss: 0.001709702901710521\n",
      "Epoch 4800, Train Loss: 0.0013012838351281043, Validation Loss: 0.001673631269667589\n",
      "Epoch 4900, Train Loss: 0.0012741214394512575, Validation Loss: 0.0016390524945651015\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for XOR:\n",
      "\n",
      "Training on Stress Classification:\n",
      "Epoch 0, Train Loss: 0.3567087181576614, Validation Loss: 0.42751056349451183\n",
      "Epoch 100, Train Loss: 0.056698221048161135, Validation Loss: 0.056014677564497344\n",
      "Epoch 200, Train Loss: 0.022578758103681915, Validation Loss: 0.022452181497249608\n",
      "Epoch 300, Train Loss: 0.013410119590476359, Validation Loss: 0.01336558909317236\n",
      "Epoch 400, Train Loss: 0.00937844605086265, Validation Loss: 0.009357761786793144\n",
      "Epoch 500, Train Loss: 0.007154582546777883, Validation Loss: 0.0071434625937550015\n",
      "Epoch 600, Train Loss: 0.005758348514618584, Validation Loss: 0.005751841657924639\n",
      "Epoch 700, Train Loss: 0.004805291544336723, Validation Loss: 0.004801290021690749\n",
      "Epoch 800, Train Loss: 0.004115629155642201, Validation Loss: 0.004113104756527665\n",
      "Epoch 900, Train Loss: 0.003594614373447489, Validation Loss: 0.0035930144739315054\n",
      "Epoch 1000, Train Loss: 0.0031877873681973898, Validation Loss: 0.003186792657374773\n",
      "Epoch 1100, Train Loss: 0.0028617099993979364, Validation Loss: 0.0028611253469609144\n",
      "Epoch 1200, Train Loss: 0.0025947577640218896, Validation Loss: 0.0025944584973739727\n",
      "Epoch 1300, Train Loss: 0.002372347564788629, Validation Loss: 0.0023722510980646845\n",
      "Epoch 1400, Train Loss: 0.002184299084289883, Validation Loss: 0.0021843490567700773\n",
      "Epoch 1500, Train Loss: 0.0020232965028141574, Validation Loss: 0.0020234534825378223\n",
      "Epoch 1600, Train Loss: 0.001883951185961284, Validation Loss: 0.001884187000462549\n",
      "Epoch 1700, Train Loss: 0.0017622087658273704, Validation Loss: 0.0017625029257340385\n",
      "Epoch 1800, Train Loss: 0.0016549618218561122, Validation Loss: 0.0016552991978797703\n",
      "Epoch 1900, Train Loss: 0.0015597897062430348, Validation Loss: 0.0015601589814195848\n",
      "Epoch 2000, Train Loss: 0.0014747794479034089, Validation Loss: 0.0014751720630674812\n",
      "Epoch 2100, Train Loss: 0.001398399773581861, Validation Loss: 0.0013988091945575333\n",
      "Epoch 2200, Train Loss: 0.0013294107699134215, Validation Loss: 0.0013298319692385291\n",
      "Epoch 2300, Train Loss: 0.0012667979759978425, Validation Loss: 0.0012672270605636777\n",
      "Epoch 2400, Train Loss: 0.0012097235462103137, Validation Loss: 0.0012101574859248283\n",
      "Epoch 2500, Train Loss: 0.0011574895486710826, Validation Loss: 0.0011579259761965721\n",
      "Epoch 2600, Train Loss: 0.001109510027863752, Validation Loss: 0.0011099470890920183\n",
      "Epoch 2700, Train Loss: 0.0010652894878695163, Validation Loss: 0.0010657257291002641\n",
      "Epoch 2800, Train Loss: 0.0010244061414347349, Validation Loss: 0.0010248404234287905\n",
      "Epoch 2900, Train Loss: 0.0009864987394665409, Validation Loss: 0.0009869301713973176\n",
      "Epoch 3000, Train Loss: 0.0009512561204582973, Validation Loss: 0.0009516840087474349\n",
      "Epoch 3100, Train Loss: 0.0009184088475076762, Validation Loss: 0.0009188326558996563\n",
      "Epoch 3200, Train Loss: 0.000887722462956373, Validation Loss: 0.0008881417811538783\n",
      "Epoch 3300, Train Loss: 0.0008589920076577905, Validation Loss: 0.0008594065265305481\n",
      "Epoch 3400, Train Loss: 0.0008320375371276214, Validation Loss: 0.0008324470290066601\n",
      "Epoch 3500, Train Loss: 0.0008067004296245541, Validation Loss: 0.0008071047325580688\n",
      "Epoch 3600, Train Loss: 0.0007828403279247885, Validation Loss: 0.0007832393330401174\n",
      "Epoch 3700, Train Loss: 0.0007603325916359601, Validation Loss: 0.0007607262329515979\n",
      "Epoch 3800, Train Loss: 0.0007390661634738997, Validation Loss: 0.0007394544096548676\n",
      "Epoch 3900, Train Loss: 0.0007189417732272512, Validation Loss: 0.000719324620889987\n",
      "Epoch 4000, Train Loss: 0.0006998704187636118, Validation Loss: 0.0007002478870223528\n",
      "Epoch 4100, Train Loss: 0.0006817720755508858, Validation Loss: 0.0006821442015632631\n",
      "Epoch 4200, Train Loss: 0.0006645745956319341, Validation Loss: 0.0006649414309521576\n",
      "Epoch 4300, Train Loss: 0.0006482127644299013, Validation Loss: 0.0006485743720172202\n",
      "Epoch 4400, Train Loss: 0.0006326274896458934, Validation Loss: 0.0006329839414066746\n",
      "Epoch 4500, Train Loss: 0.0006177651011922673, Validation Loss: 0.0006181164759580825\n",
      "Epoch 4600, Train Loss: 0.0006035767448508329, Validation Loss: 0.0006039231267139057\n",
      "Epoch 4700, Train Loss: 0.0005900178553585451, Validation Loss: 0.0005903593323009316\n",
      "Epoch 4800, Train Loss: 0.0005770476970596817, Validation Loss: 0.0005773843598245132\n",
      "Epoch 4900, Train Loss: 0.0005646289622430182, Validation Loss: 0.0005649609034057003\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABUUUlEQVR4nO3deVwV9f4/8NfMOZzDvijIJoI77prbNTM1KTSz3K7k9WtIpddcyqxu1yy3MjLTvLm2qb9scemmecslJC1TU3PfLVOhFMSURQQO58zn98eBI0dQAeecgcPr+XicB+fMfGbmPQPIy898ZkYSQggQERERuQhZ6wKIiIiI1MRwQ0RERC6F4YaIiIhcCsMNERERuRSGGyIiInIpDDdERETkUhhuiIiIyKUw3BAREZFLYbghIiIil8JwQ+Sili9fDkmScO7cOU22f+7cOUiShOXLl9tN37RpE9q2bQt3d3dIkoTMzEyMGDECUVFRTq9x27ZtkCQJ27Ztc/q2ichxGG7IpRw5cgSDBw9GZGQk3N3dER4ejgcffBDz58+3a/fmm29i3bp12hR5lywWC5YtW4YePXqgVq1aMBqNiIqKQkJCAn755Rety7utv/76C0OGDIGHhwcWLlyIFStWwMvLy+HbXbRoUamQpbUePXqgZcuWWpdRLtX5Z45qJonPliJXsXPnTvTs2RP16tVDfHw8QkJCkJqaip9//hlnzpzBb7/9Zmvr7e2NwYMHV7k/eHeSl5eHgQMHYtOmTbj//vvRr18/1KpVC+fOncPq1atx+vRppKSkoG7duli+fDkSEhJw9uxZTXpFhBAoKCiAm5sbdDodAGuvTZ8+fZCUlISYmBhb28LCQiiKAqPR6JBaWrZsicDAwFI9NIqiwGQywWAwQJad+3+9Hj164PLlyzh69KhTt1tRFfmZI6oq9FoXQKSWmTNnws/PD3v37oW/v7/dvEuXLlV6vbm5uU7pXSiPl156CZs2bcK7776LCRMm2M2bOnUq3n33XW0KK4MkSXB3d7ebVvx9uPn74+bm5qyy7MiyXKpGsuesn7nioMnvB6lCELmIpk2bih49etyxHYBSr/j4eCGEEFOnThUAxLFjx8TQoUOFv7+/aNu2rW3ZFStWiHvuuUe4u7uLgIAAERcXJ1JSUuzWf/r0aTFw4EARHBwsjEajCA8PF3FxcSIzM9PW5rvvvhNdu3YVfn5+wsvLSzRp0kRMmjTptnWnpqYKvV4vHnzwwXIdj2XLlgkA4uzZs7Zp69atEw8//LAIDQ0VBoNBNGjQQMyYMUOYzWbV9+Hs2bMCgFi2bJkQQoju3bvf8rjHx8eLyMhIuxosFouYN2+eaNmypTAajSIwMFDExsaKvXv32tosXbpU9OzZUwQFBQmDwSCaNWsmFi1aZLeeyMjIUtvt3r27EEKIrVu3CgBi69atdsusXr3a9n2uXbu2GDZsmPjjjz/s2sTHxwsvLy/xxx9/iMcee0x4eXmJwMBA8cILL5Q6nmXp3r27aNGixR3bLVy4UDRv3lwYDAYRGhoqxowZI65evWrXpqr8zJX1fRTixu9VSQDE2LFjxaeffiqaN28u9Hq9WL16tQgICBAjRowotY6srCxhNBrFCy+8YJuWn58vpkyZIho2bCgMBoOoW7eueOmll0R+fn656iXXxZ4bchmRkZHYtWsXjh49etuxDCtWrMDTTz+NTp06YdSoUQCAhg0b2rX5+9//jsaNG+PNN9+EKDpzO3PmTLz22msYMmQInn76aWRkZGD+/Pm4//77ceDAAfj7+8NkMiE2NhYFBQUYP348QkJC8Oeff+Kbb75BZmYm/Pz8cOzYMTzyyCNo3bo1ZsyYAaPRiN9++w07duy47f5t3LgRZrMZw4cPr/QxWr58Oby9vTFx4kR4e3vj+++/x5QpU5CdnY3Zs2cDgMP2YfLkyWjatCk++OADzJgxA/Xr1y913Et66qmnsHz5cvTp0wdPP/00zGYztm/fjp9//hkdOnQAACxevBgtWrTAo48+Cr1ej//9738YM2YMFEXB2LFjAQDz5s3D+PHj4e3tjcmTJwMAgoODb3uMEhIS0LFjRyQmJiI9PR3/+c9/sGPHDtv3uZjFYkFsbCw6d+6Md955B1u2bMGcOXPQsGFDPPPMM+X+vtzKtGnTMH36dMTExOCZZ57BqVOnsHjxYuzduxc7duyAm5tblf+Zu53vv/8eq1evxrhx4xAYGIjGjRtjwIAB+Oqrr/D+++/DYDDY2q5btw4FBQV4/PHHAVh7eh599FH89NNPGDVqFJo1a4YjR47g3XffxenTp6vtmDpSidbpikgt3333ndDpdEKn04kuXbqIf/3rX2Lz5s3CZDKVauvl5WXrNSip+H+YQ4cOtZt+7tw5odPpxMyZM+2mHzlyROj1etv0AwcOCABizZo1t6zz3XffFQBERkZGhfbv+eefFwDEgQMHytW+rJ6b69evl2r3z3/+U3h6etr+t6vWPtzcc1OyppK9L0KU/h//999/LwCIZ599ttR6FUW57f7ExsaKBg0a2E1r0aKFrbempJt7bkwmk6hTp45o2bKlyMvLs7X75ptvBAAxZcoUu5oBiBkzZtits127dqJ9+/altnWzO/XcXLp0SRgMBvHQQw8Ji8Vim75gwQIBQCxdulQIUbV+5iracyPLsjh27Jjd9M2bNwsA4n//+5/d9Icfftju+7pixQohy7LYvn27XbslS5YIAGLHjh3lqplcE6+WIpfx4IMPYteuXXj00Udx6NAhvP3224iNjUV4eDjWr19foXWNHj3a7vNXX30FRVEwZMgQXL582fYKCQlB48aNsXXrVgCAn58fAGDz5s24fv16mesu/p//119/DUVRyl1TdnY2AMDHx6dC+1KSh4eH7X1OTg4uX76Mbt264fr16zh58iQAx+5Def33v/+FJEmYOnVqqXmSJNnel9yfrKwsXL58Gd27d8fvv/+OrKysCm/3l19+waVLlzBmzBi7sR99+/ZFdHQ0vv3221LL3Pyz0q1bN/z+++8V3vbNtmzZApPJhAkTJtgNdh45ciR8fX1ttVT1n7nb6d69O5o3b2437YEHHkBgYCBWrVplm3b16lUkJSUhLi7ONm3NmjVo1qwZoqOj7X4nH3jgAQCw/U5SzcRwQy6lY8eO+Oqrr3D16lXs2bMHkyZNQk5ODgYPHozjx4+Xez3169e3+/zrr79CCIHGjRsjKCjI7nXixAnbQNn69etj4sSJ+OijjxAYGIjY2FgsXLjQ7g9tXFwcunbtiqeffhrBwcF4/PHHsXr16jv+0fH19QVgDSWVdezYMQwYMAB+fn7w9fVFUFAQ/u///g8AbDU6ch/K68yZMwgLC0OtWrVu227Hjh2IiYmBl5cX/P39ERQUhFdeecVufyri/PnzAICmTZuWmhcdHW2bX8zd3R1BQUF20wICAnD16tUKb7u8tRgMBjRo0MA2v6r/zN3Ozb9nAKDX6zFo0CB8/fXXKCgoAGD9z0VhYaFduPn1119x7NixUr+PTZo0AXB3FxFQ9cdwQy7JYDCgY8eOePPNN7F48WIUFhZizZo15V6+ZI8AYD2/L0kSNm3ahKSkpFKv999/39Z2zpw5OHz4MF555RXk5eXh2WefRYsWLfDHH3/Y1v3jjz9iy5YtGD58OA4fPoy4uDg8+OCDsFgst6wpOjoagPVePpWRmZmJ7t2749ChQ5gxYwb+97//ISkpCbNmzbLto6P3QU1nzpxBr169cPnyZcydOxfffvstkpKS8Pzzz5faH0cpvsRda1XlZ65kr1pJt9rGzb9nxR5//HHk5ORg48aNAIDVq1cjOjoabdq0sbVRFAWtWrUq8/cxKSkJY8aMKVfN5KK0Pi9G5GhHjhwRAMQ///lP2zRvb+/bjrm5eWzC22+/LQCIU6dOVXj7O3bsEADE5MmTb9lm5syZAoBISkq6ZZuUlBSh0+nEQw89VK7t3jzmZu3atQKA+OGHH+zaffDBB2VeMXS3+3A3Y27Gjh0rJEkSf/311y23VzyO5Pz583bTX3nllVJjjVq2bFmuMTc7d+4UAEpdcSWEEM2aNbMbS1N8tdTNyhpfUpY7jbn5/PPPBQCxYcMGu+kFBQXCz89PDBo06JbLavUz9/zzzws/P79S04cPH37Lq6XKYrFYRGhoqHj88cdFRkaG0Ov1YurUqXZtHn74YREeHm43BouoGHtuyGVs3brVdmVTSRs2bABg373v5eWFzMzMcq974MCB0Ol0mD59eqltCCHw119/AbCOUTCbzXbzW7VqBVmWbV3sV65cKbX+tm3bAoCtTVkiIiIwcuRIfPfdd6XuuAxY/yc7Z84c2//Wb1bcy1CyfpPJhEWLFtm1c+Q+lNegQYMghMD06dNLzSuuv6z9ycrKwrJly0otU97vd4cOHVCnTh0sWbLEbj82btyIEydOoG/fvhXdlUqLiYmBwWDAe++9Z7ePH3/8MbKysmy1VKWfuYYNGyIrKwuHDx+2tbl48SLWrl1bzr22kmUZgwcPxv/+9z+sWLECZrPZ7pQUAAwZMgR//vknPvzww1LL5+XlITc3t0LbJNfCS8HJZYwfPx7Xr1/HgAEDEB0dDZPJhJ07d2LVqlW2W8UXa9++PbZs2YK5c+ciLCwM9evXR+fOnW+57oYNG+KNN97ApEmTcO7cOfTv3x8+Pj44e/Ys1q5di1GjRuHFF1/E999/j3HjxuHvf/87mjRpArPZjBUrVkCn02HQoEEAgBkzZuDHH39E3759ERkZiUuXLmHRokWoW7cu7rvvvtvu45w5c3DmzBk8++yz+Oqrr/DII48gICAAKSkpWLNmDU6ePGm7VPZm9957LwICAhAfH49nn30WkiRhxYoVpcKao/ehPHr27Inhw4fjvffew6+//orevXtDURRs374dPXv2xLhx4/DQQw/BYDCgX79++Oc//4lr167hww8/RJ06dXDx4kW79bVv3x6LFy/GG2+8gUaNGqFOnTq2gaclubm5YdasWUhISED37t0xdOhQ26XgUVFRtlNeasnIyMAbb7xRanr9+vUxbNgwTJo0CdOnT0fv3r3x6KOP4tSpU1i0aBE6duxoGytVlX7mHn/8cbz88ssYMGAAnn32WVy/fh2LFy9GkyZNsH///godm7i4OMyfPx9Tp05Fq1at0KxZM7v5w4cPx+rVqzF69Ghs3boVXbt2hcViwcmTJ7F69Wps3rzZdssAqoE06zMiUtnGjRvFk08+KaKjo4W3t7cwGAyiUaNGYvz48SI9Pd2u7cmTJ8X9998vPDw8yryJ360umf3vf/8r7rvvPuHl5SW8vLxEdHS0GDt2rO101e+//y6efPJJ0bBhQ+Hu7i5q1aolevbsKbZs2WJbR3JysnjsscdEWFiYMBgMIiwsTAwdOlScPn26XPtpNpvFRx99JLp16yb8/PyEm5ubiIyMFAkJCXaX7JZ1KfiOHTvE3/72N+Hh4SHCwsJsl8ujxKkZtfbhbk5LFe/n7NmzRXR0tDAYDCIoKEj06dNH7Nu3z9Zm/fr1onXr1sLd3V1ERUWJWbNmiaVLl5ba77S0NNG3b1/h4+NTrpv4rVq1SrRr104YjUZRq1at297E72YVOS2FMm4oCUD06tXL1m7BggUiOjpauLm5ieDgYPHMM8/Y3cSvKv3MCWG9JUPLli2FwWAQTZs2FZ9++ultb+J3K4qiiIiICAFAvPHGG2W2MZlMYtasWaJFixbCaDSKgIAA0b59ezF9+nSRlZVVrn0j18RnSxEREZFL4ZgbIiIicikMN0RERORSGG6IiIjIpTDcEBERkUthuCEiIiKXwnBDRERELqXG3cRPURRcuHABPj4+t3wOChEREVUtQgjk5OQgLCwMsnz7vpkaF24uXLiAiIgIrcsgIiKiSkhNTUXdunVv26bGhRsfHx8A1oPj6+urcTVERERUHtnZ2YiIiLD9Hb+dGhduik9F+fr6MtwQERFVM+UZUsIBxURERORSGG6IiIjIpTDcEBERkUupcWNuiIjo7lksFhQWFmpdBrkYg8Fwx8u8y4PhhoiIyk0IgbS0NGRmZmpdCrkgWZZRv359GAyGu1oPww0REZVbcbCpU6cOPD09eTNUUk3xTXYvXryIevXq3dXPFsMNERGVi8VisQWb2rVra10OuaCgoCBcuHABZrMZbm5ulV4PBxQTEVG5FI+x8fT01LgSclXFp6MsFstdrYfhhoiIKoSnoshR1PrZYrghIiIil8JwQ0REVEFRUVGYN2+e1mXQLTDcEBGRy5Ik6bavadOmVWq9e/fuxahRo+6qth49emDChAl3tQ4qG6+WUou5ALh2CZAkwO/2j2InIiLnuHjxou39qlWrMGXKFJw6dco2zdvb2/ZeCAGLxQK9/s5/GoOCgtQtlFTFnhu1XDgIzGsJLH9E60qIiKhISEiI7eXn5wdJkmyfT548CR8fH2zcuBHt27eH0WjETz/9hDNnzuCxxx5DcHAwvL290bFjR2zZssVuvTeflpIkCR999BEGDBgAT09PNG7cGOvXr7+r2v/73/+iRYsWMBqNiIqKwpw5c+zmL1q0CI0bN4a7uzuCg4MxePBg27wvv/wSrVq1goeHB2rXro2YmBjk5ubeVT3VCXtu1CLrAABCMYPXERBRTSCEQF7h3V2yW1kebjrVrqz597//jXfeeQcNGjRAQEAAUlNT8fDDD2PmzJkwGo345JNP0K9fP5w6dQr16tW75XqmT5+Ot99+G7Nnz8b8+fMxbNgwnD9/HrVq1apwTfv27cOQIUMwbdo0xMXFYefOnRgzZgxq166NESNG4JdffsGzzz6LFStW4N5778WVK1ewfft2ANbeqqFDh+Ltt9/GgAEDkJOTg+3bt0MIUeljVN0w3KikQJFgBPBXTh4CtS6GiMgJ8gotaD5lsybbPj4jFp4Gdf6EzZgxAw8++KDtc61atdCmTRvb59dffx1r167F+vXrMW7cuFuuZ8SIERg6dCgA4M0338R7772HPXv2oHfv3hWuae7cuejVqxdee+01AECTJk1w/PhxzJ49GyNGjEBKSgq8vLzwyCOPwMfHB5GRkWjXrh0Aa7gxm80YOHAgIiMjAQCtWrWqcA3VGU9LqeREmrW7T7GYNa6EiIgqokOHDnafr127hhdffBHNmjWDv78/vL29ceLECaSkpNx2Pa1bt7a99/Lygq+vLy5dulSpmk6cOIGuXbvaTevatSt+/fVXWCwWPPjgg4iMjESDBg0wfPhwfPbZZ7h+/ToAoE2bNujVqxdatWqFv//97/jwww9x9erVStVRXbHnRiWSznooZSgaV0JE5BwebjocnxGr2bbV4uXlZff5xRdfRFJSEt555x00atQIHh4eGDx4MEwm023Xc/PjAiRJgqI45m+Cj48P9u/fj23btuG7777DlClTMG3aNOzduxf+/v5ISkrCzp078d1332H+/PmYPHkydu/ejfr16zuknqqG4UYlss76Q62HNuefiYicTZIk1U4NVSU7duzAiBEjMGDAAADWnpxz5845tYZmzZphx44dpepq0qQJdDprsNPr9YiJiUFMTAymTp0Kf39/fP/99xg4cCAkSULXrl3RtWtXTJkyBZGRkVi7di0mTpzo1P3Qiuv9VGpEKhpQrIMCIQRvT05EVE01btwYX331Ffr16wdJkvDaa685rAcmIyMDBw8etJsWGhqKF154AR07dsTrr7+OuLg47Nq1CwsWLMCiRYsAAN988w1+//133H///QgICMCGDRugKAqaNm2K3bt3Izk5GQ899BDq1KmD3bt3IyMjA82aNXPIPlRFDDcqkYvui6CDAosioNcx3BARVUdz587Fk08+iXvvvReBgYF4+eWXkZ2d7ZBtff755/j888/tpr3++ut49dVXsXr1akyZMgWvv/46QkNDMWPGDIwYMQIA4O/vj6+++grTpk1Dfn4+GjdujC+++AItWrTAiRMn8OOPP2LevHnIzs5GZGQk5syZgz59+jhkH6oiSdSka8MAZGdnw8/PD1lZWfD19VVtvSdPnkD0yr+hQOiB1y7BqFfvfDARUVWQn5+Ps2fPon79+nB3d9e6HHJBt/sZq8jfb14tpRJJZz2UelhgUWpUXiQiIqpSGG5UIuutA4p1koDFwiumiIiItMJwoxKd7sbwJQvvdUNERKQZhhuVSPKNcGM2M9wQERFpheFGLSXCjWIp1LAQIiKimo3hRiVCunEoLey5ISIi0gzDjUqEXc8Nww0REZFWGG7UIt24rw17boiIiLTDcKMSAQkWYb0rsYVjboiIiDRTJcLNwoULERUVBXd3d3Tu3Bl79uwp13IrV66EJEno37+/YwssJzOsvTdC4cMziYhcSY8ePTBhwgTb56ioKMybN++2y0iShHXr1t31ttVaT02iebhZtWoVJk6ciKlTp2L//v1o06YNYmNjcenSpdsud+7cObz44ovo1q2bkyq9EwFLUbixFLLnhoioKujXrx969+5d5rzt27dDkiQcPny4wuvdu3cvRo0adbfl2Zk2bRratm1bavrFixcd/lyo5cuXw9/f36HbcCbNw83cuXMxcuRIJCQkoHnz5liyZAk8PT2xdOnSWy5jsVgwbNgwTJ8+HQ0aNHBitbdnKTqcioU9N0REVcFTTz2FpKQk/PHHH6XmLVu2DB06dEDr1q0rvN6goCB4enqqUeIdhYSEwGg0OmVbrkLTcGMymbBv3z7ExMTYpsmyjJiYGOzateuWy82YMQN16tTBU0895Ywyy0WIEuFGYc8NEVFV8MgjjyAoKAjLly+3m37t2jWsWbMGTz31FP766y8MHToU4eHh8PT0RKtWrfDFF1/cdr03n5b69ddfcf/998Pd3R3NmzdHUlJSqWVefvllNGnSBJ6enmjQoAFee+01FBb19C9fvhzTp0/HoUOHIEkSJEmy1XzzaakjR47ggQcegIeHB2rXro1Ro0bh2rVrtvkjRoxA//798c477yA0NBS1a9fG2LFjbduqjJSUFDz22GPw9vaGr68vhgwZgvT0dNv8Q4cOoWfPnvDx8YGvry/at2+PX375BQBw/vx59OvXDwEBAfDy8kKLFi2wYcOGStdSHvo7N3Gcy5cvw2KxIDg42G56cHAwTp48WeYyP/30Ez7++GMcPHiwXNsoKChAQUGB7bOjHlsPlOy5YbghohpACKDwujbbdvMEJOmOzfR6PZ544gksX74ckydPhlS0zJo1a2CxWDB06FBcu3YN7du3x8svvwxfX198++23GD58OBo2bIhOnTrdcRuKomDgwIEIDg7G7t27kZWVZTc+p5iPjw+WL1+OsLAwHDlyBCNHjoSPjw/+9a9/IS4uDkePHsWmTZuwZcsWAICfn1+pdeTm5iI2NhZdunTB3r17cenSJTz99NMYN26cXYDbunUrQkNDsXXrVvz222+Ii4tD27ZtMXLkyDvuT1n7VxxsfvjhB5jNZowdOxZxcXHYtm0bAGDYsGFo164dFi9eDJ1Oh4MHD8LNzfrMxbFjx8JkMuHHH3+El5cXjh8/Dm9v7wrXURGahpuKysnJwfDhw/Hhhx8iMDCwXMskJiZi+vTpDq7MqnjMjWLmaSkiqgEKrwNvhmmz7VcuAAavcjV98sknMXv2bPzwww/o0aMHAOspqUGDBsHPzw9+fn548cUXbe3Hjx+PzZs3Y/Xq1eUKN1u2bMHJkyexefNmhIVZj8ebb75ZapzMq6++ansfFRWFF198EStXrsS//vUveHh4wNvbG3q9HiEhIbfc1ueff478/Hx88skn8PKy7v+CBQvQr18/zJo1y9ZZEBAQgAULFkCn0yE6Ohp9+/ZFcnJypcJNcnIyjhw5grNnzyIiIgIA8Mknn6BFixbYu3cvOnbsiJSUFLz00kuIjo4GADRu3Ni2fEpKCgYNGoRWrVoBgFOGk2h6WiowMBA6nc6uawsA0tPTy/zmnjlzBufOnUO/fv2g1+uh1+vxySefYP369dDr9Thz5kypZSZNmoSsrCzbKzU11SH7IgCYeVqKiKjKiY6Oxr333msby/nbb79h+/bttqENFosFr7/+Olq1aoVatWrB29sbmzdvRkpKSrnWf+LECURERNiCDQB06dKlVLtVq1aha9euCAkJgbe3N1599dVyb6Pkttq0aWMLNgDQtWtXKIqCU6dO2aa1aNECOt2N+6+Fhobe8UKd220zIiLCFmwAoHnz5vD398eJEycAABMnTsTTTz+NmJgYvPXWW3Z/j5999lm88cYb6Nq1K6ZOnVqpAdwVpWnPjcFgQPv27ZGcnGy7nFtRFCQnJ2PcuHGl2kdHR+PIkSN201599VXk5OTgP//5j92BL2Y0Gp02EEspCjeCdygmoprAzdPag6LVtivgqaeewvjx47Fw4UIsW7YMDRs2RPfu3QEAs2fPxn/+8x/MmzcPrVq1gpeXFyZMmACTyaRaubt27bJdCBMbGws/Pz+sXLkSc+bMUW0bJRWfEiomSRIURXHItgDrlV7/+Mc/8O2332Ljxo2YOnUqVq5ciQEDBuDpp59GbGwsvv32W3z33XdITEzEnDlzMH78eIfVo/lpqYkTJyI+Ph4dOnRAp06dMG/ePOTm5iIhIQEA8MQTTyA8PByJiYlwd3dHy5Yt7ZYvvnTt5unOJgRgFjpAsv4vgIjI5UlSuU8NaW3IkCF47rnn8Pnnn+OTTz7BM888Yxt/s2PHDjz22GP4v//7PwDW/2SfPn0azZs3L9e6mzVrhtTUVFy8eBGhoaEAgJ9//tmuzc6dOxEZGYnJkyfbpp0/f96ujcFguOPfj2bNmmH58uXIzc219d7s2LEDsiyjadOm5aq3oor3LzU11daJcPz4cWRmZtodoyZNmqBJkyZ4/vnnMXToUCxbtgwDBgwAAERERGD06NEYPXo0Jk2ahA8//NC1w01cXBwyMjIwZcoUpKWloW3btti0aZPtvGFKSgpkWfMr1suleEAx2HNDRFSleHt7Iy4uDpMmTUJ2djZGjBhhm9e4cWN8+eWX2LlzJwICAjB37lykp6eXO9zExMSgSZMmiI+Px+zZs5GdnW0XYoq3kZKSgpUrV6Jjx4749ttvsXbtWrs2UVFROHv2LA4ePIi6devCx8en1JmHYcOGYerUqYiPj8e0adOQkZGB8ePHY/jw4aUuzqkoi8VS6mIdo9GImJgYtGrVCsOGDcO8efNgNpsxZswYdO/eHR06dEBeXh5eeuklDB48GPXr18cff/yBvXv3YtCgQQCACRMmoE+fPmjSpAmuXr2KrVu3olmzZndV651UidQwbtw4nD9/HgUFBdi9ezc6d+5sm7dt27ZSl/CVtHz58ipz58Ybl4Iz3BARVTVPPfUUrl69itjYWLvxMa+++iruuecexMbGokePHggJCanQne9lWcbatWuRl5eHTp064emnn8bMmTPt2jz66KN4/vnnMW7cOLRt2xY7d+7Ea6+9Ztdm0KBB6N27N3r27ImgoKAyL0f39PTE5s2bceXKFXTs2BGDBw9Gr169sGDBgoodjDJcu3YN7dq1s3v169cPkiTh66+/RkBAAO6//37ExMSgQYMGWLVqFQBAp9Phr7/+whNPPIEmTZpgyJAh6NOnj+1iHovFgrFjx6JZs2bo3bs3mjRpgkWLFt11vbcjCSGEQ7dQxWRnZ8PPzw9ZWVnw9fVVbb0n07IhFnVFMzkFe+77GJ1iBqu2biKiqiA/Px9nz55F/fr14e7urnU55IJu9zNWkb/fVaLnxlWYOaCYiIhIcww3KrG/QzEHFBMREWmF4UZFxTfx44BiIiIi7TDcqKRkz43ggGIiIiLNMNyoyCKKHr/A01JE5MJq2HUo5ERq/Wwx3KioeEAx2HNDRC6o+K63169r9LBMcnnFd4Uu+eiIytD8Jn6uQkDw8QtE5NJ0Oh38/f1tzyjy9PS03eWX6G4pioKMjAx4enpCr7+7eMJwoyJz0YBijrkhIldV/FDjyj6Ekeh2ZFlGvXr17jo0M9yopOSAYp6WIiJXJUkSQkNDUadOHRQWFmpdDrkYg8GgyiOXGG5UdONqKQ4oJiLXptPp7npcBJGjcECxiorvcyOx54aIiEgzDDcqMrPnhoiISHMMNyq6MeaG4YaIiEgrDDcqEeLGTfw4oJiIiEg7DDcq4tVSRERE2mO4UYmAuBFuhKJtMURERDUYw42Kim/ix54bIiIi7TDcqEjhgGIiIiLNMdyoRIgSPTeC4YaIiEgrDDcqKh5zI/O0FBERkWYYblQiUOLxC+y5ISIi0gzDjYqKH78gM9wQERFphuFGJUIIWAQHFBMREWmN4UZFxc+WkthzQ0REpBmGGxXZngouOKCYiIhIKww3Kik5oFhSeIdiIiIirTDcqMgWbthzQ0REpBmGG5X4ebjZbuLHq6WIiIi0w3CjkoZB3hjcoZ71Ax+cSUREpBmGGxVJshsAQOZpKSIiIs0w3KhI0vG0FBERkdYYblQkyXrrV56WIiIi0gzDjYoknTXcyGDPDRERkVYYblQkycU38WO4ISIi0grDjYokXfGAYoYbIiIirTDcqKh4zI2Op6WIiIg0w3CjIllXfFqKA4qJiIi0wnCjItuAYp6WIiIi0gzDjYpkHU9LERERaY3hRkWyXNxzw9NSREREWmG4UZGs531uiIiItMZwoyJbzw3Yc0NERKQVhhsVccwNERGR9hhuVCTrrTfx0/FqKSIiIs0w3KhILnr8Ak9LERERaYfhRkW2nhueliIiItIMw42KdEXPltKz54aIiEgzDDcqKn78ggwFiiI0roaIiKhmYrhRUfFpKT0sMDPcEBERaYLhRkV6/Y373CiC4YaIiEgLDDcqKr7PjR4Ke26IiIg0wnCjIl3RaSlZErCYecUUERGRFhhuVKQrevwCAFgsZg0rISIiqrkYblRU/OBMADCbTRpWQkREVHMx3KipRM+Nwp4bIiIiTTDcqEnS2d7ytBQREZE2GG7UVLLnxlyoYSFEREQ1F8ONmmQZCiQAgGLh1VJERERaYLhRmaXokFrYc0NERKQJhhuVWWAdd8MxN0RERNpguFFZcc8Nr5YiIiLSBsONyhSGGyIiIk0x3Kis+LQUww0REZE2GG5UpkjWQyoUhhsiIiItMNyoTCkeUGxmuCEiItICw43Kik9LCZ6WIiIi0gTDjcpE0WkphaeliIiINFElws3ChQsRFRUFd3d3dO7cGXv27Lll26+++godOnSAv78/vLy80LZtW6xYscKJ1d7ejZ4b3sSPiIhIC5qHm1WrVmHixImYOnUq9u/fjzZt2iA2NhaXLl0qs32tWrUwefJk7Nq1C4cPH0ZCQgISEhKwefNmJ1deNkUqDjeKxpUQERHVTJqHm7lz52LkyJFISEhA8+bNsWTJEnh6emLp0qVltu/RowcGDBiAZs2aoWHDhnjuuefQunVr/PTTT06uvGzF4UZhzw0REZEmNA03JpMJ+/btQ0xMjG2aLMuIiYnBrl277ri8EALJyck4deoU7r///jLbFBQUIDs72+7lSMVXS/FScCIiIm1oGm4uX74Mi8WC4OBgu+nBwcFIS0u75XJZWVnw9vaGwWBA3759MX/+fDz44INltk1MTISfn5/tFRERoeo+3OxGzw3DDRERkRY0Py1VGT4+Pjh48CD27t2LmTNnYuLEidi2bVuZbSdNmoSsrCzbKzU11aG1FV8tBfbcEBERaUKv5cYDAwOh0+mQnp5uNz09PR0hISG3XE6WZTRq1AgA0LZtW5w4cQKJiYno0aNHqbZGoxFGo1HVum/nRs+NxWnbJCIiohs07bkxGAxo3749kpOTbdMURUFycjK6dOlS7vUoioKCggJHlFhhoijcsOeGiIhIG5r23ADAxIkTER8fjw4dOqBTp06YN28ecnNzkZCQAAB44oknEB4ejsTERADWMTQdOnRAw4YNUVBQgA0bNmDFihVYvHixlrthIyTeoZiIiEhLmoebuLg4ZGRkYMqUKUhLS0Pbtm2xadMm2yDjlJQUyPKNDqbc3FyMGTMGf/zxBzw8PBAdHY1PP/0UcXFxWu2CHdt9bhSeliIiItKCJIQQWhfhTNnZ2fDz80NWVhZ8fX1VX/+x2Q+hRe5u/Nh8Ou4fMkH19RMREdVEFfn7XS2vlqrKOOaGiIhIWww3KhM8LUVERKQphhuV3ei5YbghIiLSAsONyoTMxy8QERFpieFGbVLRBWgMN0RERJpguFGbzDE3REREWmK4UVnxaSnwJn5ERESaYLhRm8zTUkRERFpiuFEbx9wQERFpiuFGZaKo50ZSCjWuhIiIqGZiuFGZpCvuueGAYiIiIi0w3KhNdgMASDwtRUREpAmGG5UJHU9LERERaYnhRmWSzmD9yp4bIiIiTTDcqEwqus+NJDjmhoiISAsMN2orGnMjC56WIiIi0gLDjcokHQcUExERaYnhRmWSvrjnhqeliIiItMBwozKp6CZ+smDPDRERkRYYblRmu1qK4YaIiEgTDDcqk/TWnhsdww0REZEmGG5UJsscc0NERKQlhhuV3RhQzJ4bIiIiLTDcqExXFG54WoqIiEgbDDcqk/TWAcU8LUVERKQNhhuVyUUPztSBPTdERERaYLhRma6o50bPnhsiIiJNMNyoTC56/AJ7boiIiLTBcKOyGwOKFY0rISIiqpkYblSmcys6LcWeGyIiIk0w3KhMLuq50YNjboiIiLTAcKMy24BimCGE0LgaIiKimofhRmX64jE3UGBWGG6IiIicjeFGZcU9N26wwGxhuCEiInI2hhuV6dyKx9yYUajwiikiIiJnY7hRmZubEQCgkwQsZg4qJiIicjaGG5UVP34BAArNJg0rISIiqpkYbtRWdIdiALAUMtwQERE5G8ON2uQS4cZcqGEhRERENRPDjdrkG6elzIUMN0RERM7GcKM2WYal6LBaOOaGiIjI6RhuHMAMHQCeliIiItICw40DFIcbhT03RERETsdw4wBKUbgxs+eGiIjI6RhuHMAsWQcVC4YbIiIip2O4cQALT0sRERFphuHGARSpaECxxaxxJURERDUPw40DWGA9LaVY2HNDRETkbAw3DmApGnOjcMwNERGR01Uq3KSmpuKPP/6wfd6zZw8mTJiADz74QLXCqjOLVDzmhuGGiIjI2SoVbv7xj39g69atAIC0tDQ8+OCD2LNnDyZPnowZM2aoWmB1pBRfLWVhuCEiInK2SoWbo0ePolOnTgCA1atXo2XLlti5cyc+++wzLF++XM36qiWFl4ITERFpplLhprCwEEajEQCwZcsWPProowCA6OhoXLx4Ub3qqqniq6UU9twQERE5XaXCTYsWLbBkyRJs374dSUlJ6N27NwDgwoULqF27tqoFVke2nhuFl4ITERE5W6XCzaxZs/D++++jR48eGDp0KNq0aQMAWL9+ve10VU3GMTdERETa0VdmoR49euDy5cvIzs5GQECAbfqoUaPg6empWnHVlZA55oaIiEgrleq5ycvLQ0FBgS3YnD9/HvPmzcOpU6dQp04dVQusjop7bqAw3BARETlbpcLNY489hk8++QQAkJmZic6dO2POnDno378/Fi9erGqB1ZGt54ZjboiIiJyuUuFm//796NatGwDgyy+/RHBwMM6fP49PPvkE7733nqoFVkc8LUVERKSdSoWb69evw8fHBwDw3XffYeDAgZBlGX/7299w/vx5VQusjorDDTigmIiIyOkqFW4aNWqEdevWITU1FZs3b8ZDDz0EALh06RJ8fX1VLbBa0rkBAATH3BARETldpcLNlClT8OKLLyIqKgqdOnVCly5dAFh7cdq1a6dqgdWSbA03sHDMDRERkbNV6lLwwYMH47777sPFixdt97gBgF69emHAgAGqFVdt2cKNSds6iIiIaqBKhRsACAkJQUhIiO3p4HXr1uUN/IoInfXRFBLDDRERkdNV6rSUoiiYMWMG/Pz8EBkZicjISPj7++P111+Hoihq11j9FI25kRSGGyIiImerVM/N5MmT8fHHH+Ott95C165dAQA//fQTpk2bhvz8fMycOVPVIqsbSW8AAMgcUExEROR0lQo3/+///T989NFHtqeBA0Dr1q0RHh6OMWPG1PhwA9tpKYYbIiIiZ6vUaakrV64gOjq61PTo6GhcuXLlrouq7m703PC0FBERkbNVKty0adMGCxYsKDV9wYIFaN26dYXXt3DhQkRFRcHd3R2dO3fGnj17btn2ww8/RLdu3RAQEICAgADExMTctr0WeFqKiIhIO5U6LfX222+jb9++2LJli+0eN7t27UJqaio2bNhQoXWtWrUKEydOxJIlS9C5c2fMmzcPsbGxt3wI57Zt2zB06FDce++9cHd3x6xZs/DQQw/h2LFjCA8Pr8zuqE7SW09LyYLhhoiIyNkq1XPTvXt3nD59GgMGDEBmZiYyMzMxcOBAHDt2DCtWrKjQuubOnYuRI0ciISEBzZs3x5IlS+Dp6YmlS5eW2f6zzz7DmDFj0LZtW0RHR+Ojjz6CoihITk6uzK44hFzUc6Njzw0REZHTVfo+N2FhYaUGDh86dAgff/wxPvjgg3Ktw2QyYd++fZg0aZJtmizLiImJwa5du8q1juvXr6OwsBC1atUqc35BQQEKCgpsn7Ozs8u13rthCzfsuSEiInK6SvXcqOXy5cuwWCwIDg62mx4cHIy0tLRyrePll19GWFgYYmJiypyfmJgIPz8/2ysiIuKu674T2c16WorhhoiIyPk0DTd366233sLKlSuxdu1auLu7l9lm0qRJyMrKsr1SU1MdXpdcNOZGz3BDRETkdJU+LaWGwMBA6HQ6pKen201PT09HSEjIbZd955138NZbb2HLli23vULLaDTCaDSqUm956d2Kww0fnElERORsFQo3AwcOvO38zMzMCm3cYDCgffv2SE5ORv/+/QHANjh43Lhxt1zu7bffxsyZM7F582Z06NChQtt0Bp6WIiIi0k6Fwo2fn98d5z/xxBMVKmDixImIj49Hhw4d0KlTJ8ybNw+5ublISEgAADzxxBMIDw9HYmIiAGDWrFmYMmUKPv/8c0RFRdnG5nh7e8Pb27tC23YUnZt1QLEbGG6IiIicrULhZtmyZaoXEBcXh4yMDEyZMgVpaWlo27YtNm3aZBtknJKSAlm+MTRo8eLFMJlMGDx4sN16pk6dimnTpqleX2XoeFqKiIhIM5IQQmhdhDNlZ2fDz88PWVlZ8PX1dcg2/jqzH7VX9ESG8EPQ9BSHbIOIiKgmqcjf72p9tVRVpTdYr9wyoBAWpUZlRyIiIs0x3DiA3mA9LeUGCwotisbVEBER1SwMNw5wI9yYYWK4ISIiciqGGwdwc7OelnKTLCgs5KBiIiIiZ2K4cYDiZ0sBQKGp4DYtiYiISG0MN46gv3FHZDPDDRERkVMx3DiC7GZ7W1iYr2EhRERENQ/DjSPIMszQAQDMJoYbIiIiZ2K4cRATrL035kKeliIiInImhhsHMRc92cLCMTdEREROxXDjIGapKNyw54aIiMipGG4cxCxZT0sx3BARETkXw42DMNwQERFpg+HGQSxFp6UUhhsiIiKnYrhxELNkvUuxYma4ISIiciaGGwdRbAOKTRpXQkREVLMw3DiIRWbPDRERkRYYbhxEKXoEA8fcEBERORfDjYOI4nDDnhsiIiKnYrhxEEVnfTK4YM8NERGRUzHcOIiic7e+MfPBmURERM7EcOMgQm/tuWG4ISIici6GGwcRRT03EsMNERGRUzHcOEpRz41k4ZgbIiIiZ2K4cRQ3DwCAzJ4bIiIip2K4cRBJbz0tJbPnhoiIyKkYbhzFYA03OoU9N0RERM7EcOMgOr31tJTOwmdLERERORPDjYNIhqJwo/C0FBERkTMx3DiIrui0lF5hzw0REZEzMdw4iK6o58ZNsOeGiIjImRhuHERv8LR+Fey5ISIiciaGGwfRGa09Nwb23BARETkVw42DuBX13BhEocaVEBER1SwMNw7i5m7tuTGiAEIIjashIiKqORhuHMRgtPbcGFEIs8JwQ0RE5CwMNw7i5lEcbkwoMCsaV0NERFRzMNw4iNG9aMyNZEFBAa+YIiIichaGGweRip4KDgAFBdc1rISIiKhmYbhxlKKnggOAKY/hhoiIyFkYbhxF1qEQegCAKT9P42KIiIhqDoYbBzLBzfo1P1fjSoiIiGoOhhsHMklG61f23BARETkNw40DFUoG69cC9twQERE5C8ONAxXI1iumzPk5GldCRERUczDcOFChzhpuLBxzQ0RE5DQMNw5kLgo3SsE1jSshIiKqORhuHMgWbkzsuSEiInIWhhsHsuitj2AQDDdEREROw3DjQKIo3MDEOxQTERE5C8ONAylu1nAjF7LnhoiIyFkYbhypKNxIhbyJHxERkbMw3DiS0RsAoDPztBQREZGzMNw4kGzwAgDoLOy5ISIichaGGweSjdZwo2e4ISIichqGGwfSu1vDjUFhuCEiInIWhhsH0rtbx9ww3BARETkPw40D6d19AABGJV/jSoiIiGoOhhsHMnhYe26MguGGiIjIWRhuHMjoaQ037iiAEELjaoiIiGoGhhsH8vDyAwB4IR/5hYrG1RAREdUMDDcO5OFlHXPjIZmQc52npoiIiJyB4caBJHdf2/trOZnaFUJERFSDMNw4kt6IArgBAPJyrmpcDBERUc3AcONg1yXrjfzyrzHcEBEROQPDjYPlydZwU5ibqW0hRERENQTDjYMV6BhuiIiInEnzcLNw4UJERUXB3d0dnTt3xp49e27Z9tixYxg0aBCioqIgSRLmzZvnvEIryaS33uvGnJelcSVEREQ1g6bhZtWqVZg4cSKmTp2K/fv3o02bNoiNjcWlS5fKbH/9+nU0aNAAb731FkJCQpxcbeUUulkvBxf5DDdERETOoGm4mTt3LkaOHImEhAQ0b94cS5YsgaenJ5YuXVpm+44dO2L27Nl4/PHHYTQanVxt5VgM1nCD/GxtCyEiIqohNAs3JpMJ+/btQ0xMzI1iZBkxMTHYtWuXVmWpThis97qRCnI0roSIiKhm0Gu14cuXL8NisSA4ONhuenBwME6ePKnadgoKClBQUGD7nJ3t5B4UozXc6ArZc0NEROQMmg8odrTExET4+fnZXhEREU7dvuxhDTf6wmtO3S4REVFNpVm4CQwMhE6nQ3p6ut309PR0VQcLT5o0CVlZWbZXamqqausuD52HPwDAYGa4ISIicgbNwo3BYED79u2RnJxsm6YoCpKTk9GlSxfVtmM0GuHr62v3cia9p/XJ4EYLww0REZEzaDbmBgAmTpyI+Ph4dOjQAZ06dcK8efOQm5uLhIQEAMATTzyB8PBwJCYmArAOQj5+/Ljt/Z9//omDBw/C29sbjRo10mw/bsfg7Q8A8LDkalsIERFRDaFpuImLi0NGRgamTJmCtLQ0tG3bFps2bbINMk5JSYEs3+hcunDhAtq1a2f7/M477+Cdd95B9+7dsW3bNmeXXy6ePrWsX0UuhBCQJEnjioiIiFybJIQQWhfhTNnZ2fDz80NWVpZTTlFdv3ganu93xDXhDnnyn/A0aJoniYiIqqWK/P12+aultObhHwQA8JbycSWLl4MTERE5GsONg0nu/jAXHeZrVzM0roaIiMj1Mdw4miQhW7J2n13PTL9DYyIiIrpbDDdOkKuzXg6el1X2A0GJiIhIPQw3TpDn5g8AKMz5S9tCiIiIagCGGycwGfwBAEruZW0LISIiqgEYbpzAYgywvsllzw0REZGjMdw4gfCsDQDQ5V/VuBIiIiLXx3DjBFJRuHEzMdwQERE5GsONE7j5WG/k527K1LYQIiKiGoDhxgk8/OsAALzMmdoWQkREVAMw3DiBX1A4ACBAXIVFqVGP8iIiInI6hhsn8KtTDwAQiCz8lZOrcTVERESujeHGCXTeQTBDhiwJXEn/U+tyiIiIXBrDjTPIMq7KtQAAORkpGhdDRETk2hhunCRHHwgAyLvCnhsiIiJHYrhxkjx36+Xg5syLGldCRETk2hhunKTQMxgAIOUw3BARETkSw42z+IQAAPTX0zUuhIiIyLUx3DiJMcB6rxvPfIYbIiIiR2K4cRLf0AYAgNqFaRCCN/IjIiJyFIYbJ6ldNxoAEIoMZOXma1wNERGR62K4cRL32hEwQwejZEbaH+e0LoeIiMhlMdw4i6xDhs76AM2rF37VuBgiIiLXxXDjRFnGMABAfsbvGldCRETkuhhunCjfO8L65so5TesgIiJyZQw3TiQFRAIADDl8vhQREZGjMNw4kVdoEwCAfx7DDRERkaMw3DhRUIO2AIBIJRXX8gu1LYaIiMhFMdw4kV94UxRCD28pHym/n9K6HCIiIpfEcONMOjdc1NcFAFw5d1jjYoiIiFwTw42TZXk3BACYLh7VuBIiIiLXxHDjZJbApgAAw5XTGldCRETkmhhunMwnqh0AICT3FB+gSURE5AAMN04W3rwrAKCBSMWf6RkaV0NEROR6GG6czL1WOC7JQZAlgdRjO7Uuh4iIyOUw3Ggg3acFACD/7G6NKyEiInI9DDcaUMLuAQB4ZhzUthAiIiIXxHCjgaDm9wMAGucfwfUCk8bVEBERuRaGGw2ENuuK63BHLSkHJw7s0LocIiIil8JwowFJb8A5b+upqaxjSRpXQ0RE5FoYbjRiqd8dAOB/kT03REREamK40UhEp8cAAC0LjyD1zwsaV0NEROQ6GG404h/RDKn6KBgkC37bvkrrcoiIiFwGw42GrkT1BQD4/P6NxpUQERG5DoYbDUXc9w8AQJuCAzh77qzG1RAREbkGhhsN1YpqiTPGaLhJFpxNWqJ1OURERC6B4UZjprYJAICmf/4X+byhHxER0V1juNFYkweeQBa8EY4M7P52qdblEBERVXsMNxrTGT1xrtFwAEDEkQUoMLH3hoiI6G4w3FQB0f1fRja80ECkYtfXH2hdDhERUbXGcFMFGL0DcLbJUwCAFkdnIyPjksYVERERVV8MN1VEy8GT8acuHEFSJo5/+iKEEFqXREREVC0x3FQROoM7Ch6aDQDonvU1dnz7icYVERERVU8MN1VIg859cTB8GACg1d5JOH38oLYFERERVUMMN1VM6xHv4jdDNPykXHiu/jv+TD2ndUlERETVCsNNFSO7GRH8z69wQQ5FXVyCaekjOH/2tNZlERERVRsMN1WQT+1wGEasQ4ZUG/VFKoz/rw+OHdildVlERETVAsNNFRVYLxry098hVa6LEFxGw3X9sH3lOxCKonVpREREVRrDTRVWO7wR/Md/j2OeneAuFaLbyddx8K0Hcfb0Ea1LIyIiqrIYbqo4n4BgNH9xE/Y3mQCT0KOd6ReEftYT2xf+E2l/pmhdHhERUZUjiRp2t7js7Gz4+fkhKysLvr6+WpdTIem/H8Ffq59F8/z9AIA8YcCB2o8gsOczaNKqk8bVEREROU5F/n4z3FQ3QuDET2uh+3EWmhSetE0+pm+BrEaPoUG3xxESHqlhgUREROpjuLmNah9uighFwemfv0Hezg/RMucn6CXrQGNFSDhhaI7MsO4IaBmDxm27wc3NoHG1REREd4fh5jZcJdyUdCXtPM5s+Rj+5zehceEpu3k5wgNnPFrhelAbuEd2QN3m96JOWD2NKiUiIqochpvbcMVwU9LlP8/g7I4v4ZayHQ2u7Ycvcku1SUctpBujkOvbEFJgE3iFN0dw/VYIDImArOMYcyIiqnoYbm7D1cNNSYrZjHPHduHyyZ8gXziAoJzjiLD8AVkq+1teINxwSQ5CpiEY1z3CoPiGQxcQCWNAKLxqh8E/MBz+gSHQ8zQXERE5GcPNbdSkcFOWvJxMpJ78BVmpR2HJOA2PzDMIzD+HECUduluEnpIUISFT8kGWHIBctwDkG2rBYvCFYvQHPPwhe/rDzSsABu9acPepDU+/QHj51oaXjx/0bm6O30EiInJJ1S7cLFy4ELNnz0ZaWhratGmD+fPno1OnW1/avGbNGrz22ms4d+4cGjdujFmzZuHhhx8u17Zqeri5lUJTPjL+PIvMi7/j+qWzMF9JgS7nD3hcvwCvwqvwVa4iQGTfstenPPKEAXmSO/IldxRIHjDJHjDpPGDWecKs94Ti5gmh94QweENy8wDc3CG7eUAq+qozuBe9POBm9ITe6AE3gwfc3D1gMHrC4O4Bdw9v6PR6FY8MERFVBRX5+635X4FVq1Zh4sSJWLJkCTp37ox58+YhNjYWp06dQp06dUq137lzJ4YOHYrExEQ88sgj+Pzzz9G/f3/s378fLVu21GAPXIObwR1h9ZshrH6zW7axmAvx118XkZ1xAblXLqIgMw2FORkQ+VmQ8zOhK8iCW2E2DOYceFhy4KVcg7fIhYdkAgB4SCZ4wASIbEAAUACY1d+XQqFDIfQwS3qYoYO56L0Felgk+5ci6WGR3SCK3iuyG4SshyIbIGQ9IOshdMXv3QCdGyDJgKyDJOsBWQdIOkg6/Y2vsg5S0XtJLv56471cYpqsK55mfS/r3Iq+Fk/TQ9brodPpIckyJEmGLOsgy9YaZFkueukgyTfmybIOkiRBkjmGiohqHs17bjp37oyOHTtiwYIFAABFURAREYHx48fj3//+d6n2cXFxyM3NxTfffGOb9re//Q1t27bFkiVL7rg99tw4X0H+deRdy0ZebhYKruegIDcbhXk5KMzLgSX/GiwF1yAKrkGYrkMyXYNUmAvZnA9ZKYBsKYBOMUGvFBS9THATJriJQrjBBKMwwQATDJJF692skhQhQYH1JSAXfZWgQIYi3Xhf8qvde0m2LSukoq/F6yr5WboxvbgdJAnWf1wkAFLR9BvvbdMBQCqeLhe1KVquxDLWydY2KKrTuhxKr1MqCnXF06SS7YuWKVpPqXk3tbGuV7Ztv2Q7qXhbdsuiaJ4MUbSMZGsDu5pLfb7dPACSdOc2JeuXythm8TRR1rpvqkO61bZK1CGVqkMuUYb9eoRtXnFbWI/NzesuuTrIt9yW/fGQ7Ze76XtRarmbPpfaj5vboKz/LJTcD6nE5FtMv1V7oOjnsfQ8+8Vv0QZlb7v0MiWbSWW3KVmjfNO6UPb2UUa9bkYPBIaoe2Vutem5MZlM2LdvHyZNmmSbJssyYmJisGtX2U/B3rVrFyZOnGg3LTY2FuvWrSuzfUFBAQoKCmyfs7Oz775wqhCjuyeM7p7wDwxx2DYsZjNMBXkw5V8v+poHi7kAFnMhLIUFsBSaoFis7xVzIRRLIYS56L3ZBFiKp5kApRDCYgYsJkiWQghLISSlEJLFBAgLJGEBFAskoUASZruvEArkoveybZ4CWVggw7qMDAvkm77qij9DgQ4lpynQF02XIMo1LqokWbLGj6KjVPEDK27xnojoNk7qmyHw1Z81276m4eby5cuwWCwIDg62mx4cHIyTJ0+WuUxaWlqZ7dPS0spsn5iYiOnTp6tTMFVZOr0eHnofeHj5aF2KwwlFgaIoUBQLFMVS9NlSNE2xfraYrfOEYpsvSswXigVCWKAoAlAsUETJ6UXvhXU5FL0XxcsKC4QiAGGxtRNCsbWDsECxKAAUQAgICEAUvxSI4vcQ1vcQkMSN90JRANxoU/KrsL1Xypxv3R4AoVjXieJ2KNXeus2btwUA1mWLDnapbUglP9/URipVd1HflFCKv3vWabYO85sSY/E6Ss4ruZ7i6bbFbppXxnpvnnfr9Zdcruw6pZLrLbWtkuspe19KzruxvtLrvn3dpfdBEjetH7fZdont3diHW6wXZR0b2G3Tft327cquubTytLtVm1ttuzLL3HqfKl6jRdb2AhLNx9w42qRJk+x6erKzsxEREaFhRUR3R5Jl6GQZOtf/9SWiaqqFxtvX9F/HwMBA6HQ6pKen201PT09HSEjZpzBCQkIq1N5oNMJoNKpTMBEREVV5ml5KYTAY0L59eyQnJ9umKYqC5ORkdOnSpcxlunTpYtceAJKSkm7ZnoiIiGoWzfu1J06ciPj4eHTo0AGdOnXCvHnzkJubi4SEBADAE088gfDwcCQmJgIAnnvuOXTv3h1z5sxB3759sXLlSvzyyy/44IMPtNwNIiIiqiI0DzdxcXHIyMjAlClTkJaWhrZt22LTpk22QcMpKSnWe3oUuffee/H555/j1VdfxSuvvILGjRtj3bp1vMcNERERAYD297lxNt7nhoiIqPqpyN9v3r6UiIiIXArDDREREbkUhhsiIiJyKQw3RERE5FIYboiIiMilMNwQERGRS2G4ISIiIpfCcENEREQuheGGiIiIXIrmj19wtuIbMmdnZ2tcCREREZVX8d/t8jxYocaFm5ycHABARESExpUQERFRReXk5MDPz++2bWrcs6UURcGFCxfg4+MDSZJUXXd2djYiIiKQmprK51Y5EI+zc/A4OwePs/PwWDuHo46zEAI5OTkICwuze6B2WWpcz40sy6hbt65Dt+Hr68tfHCfgcXYOHmfn4HF2Hh5r53DEcb5Tj00xDigmIiIil8JwQ0RERC6F4UZFRqMRU6dOhdFo1LoUl8bj7Bw8zs7B4+w8PNbOURWOc40bUExERESujT03RERE5FIYboiIiMilMNwQERGRS2G4ISIiIpfCcKOShQsXIioqCu7u7ujcuTP27NmjdUlV2o8//oh+/fohLCwMkiRh3bp1dvOFEJgyZQpCQ0Ph4eGBmJgY/Prrr3Ztrly5gmHDhsHX1xf+/v546qmncO3aNbs2hw8fRrdu3eDu7o6IiAi8/fbbjt61KiUxMREdO3aEj48P6tSpg/79++PUqVN2bfLz8zF27FjUrl0b3t7eGDRoENLT0+3apKSkoG/fvvD09ESdOnXw0ksvwWw227XZtm0b7rnnHhiNRjRq1AjLly939O5VGYsXL0br1q1tNy3r0qULNm7caJvPY+wYb731FiRJwoQJE2zTeKzv3rRp0yBJkt0rOjraNr9aHGNBd23lypXCYDCIpUuXimPHjomRI0cKf39/kZ6ernVpVdaGDRvE5MmTxVdffSUAiLVr19rNf+utt4Sfn59Yt26dOHTokHj00UdF/fr1RV5enq1N7969RZs2bcTPP/8stm/fLho1aiSGDh1qm5+VlSWCg4PFsGHDxNGjR8UXX3whPDw8xPvvv++s3dRcbGysWLZsmTh69Kg4ePCgePjhh0W9evXEtWvXbG1Gjx4tIiIiRHJysvjll1/E3/72N3Hvvffa5pvNZtGyZUsRExMjDhw4IDZs2CACAwPFpEmTbG1+//134enpKSZOnCiOHz8u5s+fL3Q6ndi0aZNT91cr69evF99++604ffq0OHXqlHjllVeEm5ubOHr0qBCCx9gR9uzZI6KiokTr1q3Fc889Z5vOY333pk6dKlq0aCEuXrxoe2VkZNjmV4djzHCjgk6dOomxY8faPlssFhEWFiYSExM1rKr6uDncKIoiQkJCxOzZs23TMjMzhdFoFF988YUQQojjx48LAGLv3r22Nhs3bhSSJIk///xTCCHEokWLREBAgCgoKLC1efnll0XTpk0dvEdV16VLlwQA8cMPPwghrMfVzc1NrFmzxtbmxIkTAoDYtWuXEMIaRGVZFmlpabY2ixcvFr6+vrZj+69//Uu0aNHCbltxcXEiNjbW0btUZQUEBIiPPvqIx9gBcnJyROPGjUVSUpLo3r27LdzwWKtj6tSpok2bNmXOqy7HmKel7pLJZMK+ffsQExNjmybLMmJiYrBr1y4NK6u+zp49i7S0NLtj6ufnh86dO9uO6a5du+Dv748OHTrY2sTExECWZezevdvW5v7774fBYLC1iY2NxalTp3D16lUn7U3VkpWVBQCoVasWAGDfvn0oLCy0O9bR0dGoV6+e3bFu1aoVgoODbW1iY2ORnZ2NY8eO2dqUXEdxm5r4O2CxWLBy5Urk5uaiS5cuPMYOMHbsWPTt27fU8eCxVs+vv/6KsLAwNGjQAMOGDUNKSgqA6nOMGW7u0uXLl2GxWOy+iQAQHByMtLQ0jaqq3oqP2+2OaVpaGurUqWM3X6/Xo1atWnZtylpHyW3UJIqiYMKECejatStatmwJwHocDAYD/P397drefKzvdBxv1SY7Oxt5eXmO2J0q58iRI/D29obRaMTo0aOxdu1aNG/enMdYZStXrsT+/fuRmJhYah6PtTo6d+6M5cuXY9OmTVi8eDHOnj2Lbt26IScnp9oc4xr3VHCimmrs2LE4evQofvrpJ61LcUlNmzbFwYMHkZWVhS+//BLx8fH44YcftC7LpaSmpuK5555DUlIS3N3dtS7HZfXp08f2vnXr1ujcuTMiIyOxevVqeHh4aFhZ+bHn5i4FBgZCp9OVGimenp6OkJAQjaqq3oqP2+2OaUhICC5dumQ332w248qVK3ZtylpHyW3UFOPGjcM333yDrVu3om7durbpISEhMJlMyMzMtGt/87G+03G8VRtfX99q84/h3TIYDGjUqBHat2+PxMREtGnTBv/5z394jFW0b98+XLp0Cffccw/0ej30ej1++OEHvPfee9Dr9QgODuaxdgB/f380adIEv/32W7X5eWa4uUsGgwHt27dHcnKybZqiKEhOTkaXLl00rKz6ql+/PkJCQuyOaXZ2Nnbv3m07pl26dEFmZib27dtna/P9999DURR07tzZ1ubHH39EYWGhrU1SUhKaNm2KgIAAJ+2NtoQQGDduHNauXYvvv/8e9evXt5vfvn17uLm52R3rU6dOISUlxe5YHzlyxC5MJiUlwdfXF82bN7e1KbmO4jY1+XdAURQUFBTwGKuoV69eOHLkCA4ePGh7dejQAcOGDbO957FW37Vr13DmzBmEhoZWn59nVYYl13ArV64URqNRLF++XBw/flyMGjVK+Pv7240UJ3s5OTniwIED4sCBAwKAmDt3rjhw4IA4f/68EMJ6Kbi/v7/4+uuvxeHDh8Vjjz1W5qXg7dq1E7t37xY//fSTaNy4sd2l4JmZmSI4OFgMHz5cHD16VKxcuVJ4enrWqEvBn3nmGeHn5ye2bdtmd1nn9evXbW1Gjx4t6tWrJ77//nvxyy+/iC5duoguXbrY5hdf1vnQQw+JgwcPik2bNomgoKAyL+t86aWXxIkTJ8TChQtr1KWz//73v8UPP/wgzp49Kw4fPiz+/e9/C0mSxHfffSeE4DF2pJJXSwnBY62GF154QWzbtk2cPXtW7NixQ8TExIjAwEBx6dIlIUT1OMYMNyqZP3++qFevnjAYDKJTp07i559/1rqkKm3r1q0CQKlXfHy8EMJ6Ofhrr70mgoODhdFoFL169RKnTp2yW8dff/0lhg4dKry9vYWvr69ISEgQOTk5dm0OHTok7rvvPmE0GkV4eLh46623nLWLVUJZxxiAWLZsma1NXl6eGDNmjAgICBCenp5iwIAB4uLFi3brOXfunOjTp4/w8PAQgYGB4oUXXhCFhYV2bbZu3Sratm0rDAaDaNCggd02XN2TTz4pIiMjhcFgEEFBQaJXr162YCMEj7Ej3RxueKzvXlxcnAgNDRUGg0GEh4eLuLg48dtvv9nmV4djLAkhhDp9QERERETa45gbIiIicikMN0RERORSGG6IiIjIpTDcEBERkUthuCEiIiKXwnBDRERELoXhhoiIiFwKww0R1UiSJGHdunVal0FEDsBwQ0RON2LECEiSVOrVu3dvrUsjIheg17oAIqqZevfujWXLltlNMxqNGlVDRK6EPTdEpAmj0YiQkBC7V/HT2iVJwuLFi9GnTx94eHigQYMG+PLLL+2WP3LkCB544AF4eHigdu3aGDVqFK5du2bXZunSpWjRogWMRiNCQ0Mxbtw4u/mXL1/GgAED4OnpicaNG2P9+vW2eVevXsWwYcMQFBQEDw8PNG7cuFQYI6KqieGGiKqk1157DYMGDcKhQ4cwbNgwPP744zhx4gQAIDc3F7GxsQgICMDevXuxZs0abNmyxS68LF68GGPHjsWoUaNw5MgRrF+/Ho0aNbLbxvTp0zFkyBAcPnwYDz/8MIYNG4YrV67Ytn/8+HFs3LgRJ06cwOLFixEYGOi8A0BElafaIziJiMopPj5e6HQ64eXlZfeaOXOmEML6NPPRo0fbLdO5c2fxzDPPCCGE+OCDD0RAQIC4du2abf63334rZFkWaWlpQgghwsLCxOTJk29ZAwDx6quv2j5fu3ZNABAbN24UQgjRr18/kZCQoM4OE5FTccwNEWmiZ8+eWLx4sd20WrVq2d536dLFbl6XLl1w8OBBAMCJEyfQpk0beHl52eZ37doViqLg1KlTkCQJFy5cQK9evW5bQ+vWrW3vvby84Ovri0uXLgEAnnnmGQwaNAj79+/HQw89hP79++Pee++t1L4SkXMx3BCRJry8vEqdJlKLh4dHudq5ubnZfZYkCYqiAAD69OmD8+fPY8OGDUhKSkKvXr0wduxYvPPOO6rXS0Tq4pgbIqqSfv7551KfmzVrBgBo1qwZDh06hNzcXNv8HTt2QJZlNG3aFD4+PoiKikJycvJd1RAUFIT4+Hh8+umnmDdvHj744IO7Wh8ROQd7bohIEwUFBUhLS7ObptfrbYN216xZgw4dOuC+++7DZ599hj179uDjjz8GAAwbNgxTp05FfHw8pk2bhoyMDIwfPx7Dhw9HcHAwAGDatGkYPXo06tSpgz59+iAnJwc7duzA+PHjy1XflClT0L59e7Ro0QIFBQX45ptvbOGKiKo2hhsi0sSmTZsQGhpqN61p06Y4efIkAOuVTCtXrsSYMWMQGhqKL774As2bNwcAeHp6YvPmzXjuuefQsWNHeHp6YtCgQZg7d65tXfHx8cjPz8e7776LF198EYGBgRg8eHC56zMYDJg0aRLOnTsHDw8PdOvWDStXrlRhz4nI0SQhhNC6CCKikiRJwtq1a9G/f3+tSyGiaohjboiIiMilMNwQERGRS+GYGyKqcni2nIjuBntuiIiIyKUw3BAREZFLYbghIiIil8JwQ0RERC6F4YaIiIhcCsMNERERuRSGGyIiInIpDDdERETkUhhuiIiIyKX8fxPRAYgfHJR8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Stress Classification:\n",
      "Test Data for XOR Network:\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 1]] [[1]\n",
      " [1]\n",
      " [0]] [[1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Test Data for Stress Classification:\n",
      "[[ 60.82241993  50.82241993]\n",
      " [ 60.71791889  50.71791889]\n",
      " [120.77169871  32.15145738]\n",
      " [ 59.74133885  49.74133885]\n",
      " [119.16027816  34.40060735]\n",
      " [ 59.63238016  49.63238016]\n",
      " [119.44635069  33.80212211]\n",
      " [ 59.96597259  49.96597259]\n",
      " [121.03184454  33.51443963]\n",
      " [118.62809886  33.3864386 ]\n",
      " [ 59.34502442  49.34502442]\n",
      " [120.70775194  34.43753322]\n",
      " [ 59.53345469  49.53345469]\n",
      " [ 58.99255603  48.99255603]\n",
      " [118.30753537  36.52955032]\n",
      " [119.12201741  34.17311965]\n",
      " [ 61.77858758  51.77858758]\n",
      " [ 59.13084394  49.13084394]\n",
      " [ 61.08724594  51.08724594]\n",
      " [ 59.81482446  49.81482446]\n",
      " [118.8086965   35.65655361]\n",
      " [120.61277391  33.94658444]\n",
      " [ 59.42794256  49.42794256]\n",
      " [120.28328787  34.74109503]\n",
      " [120.20292302  33.48425589]\n",
      " [ 59.48470568  49.48470568]\n",
      " [ 60.47004215  50.47004215]\n",
      " [119.9803622   35.55248995]\n",
      " [119.31184965  37.25243581]\n",
      " [119.02412675  36.0536418 ]\n",
      " [ 60.03143917  50.03143917]\n",
      " [118.00626436  35.37405657]\n",
      " [120.03408347  34.23002677]\n",
      " [ 60.98499313  50.98499313]\n",
      " [119.37735064  35.19460746]\n",
      " [120.78580016  35.42545756]\n",
      " [120.71161488  33.87535791]\n",
      " [119.68473076  35.75896922]\n",
      " [ 59.24321132  49.24321132]\n",
      " [120.18186626  35.24822059]\n",
      " [ 60.12765468  50.12765468]\n",
      " [119.51115058  33.88038257]\n",
      " [119.21221625  34.3791524 ]\n",
      " [119.63755906  33.88033011]\n",
      " [ 61.29314154  51.29314154]\n",
      " [ 61.83168225  51.83168225]\n",
      " [118.65181458  35.74326409]\n",
      " [ 59.52364223  49.52364223]\n",
      " [120.2597225   34.09568337]\n",
      " [120.19584526  34.02162722]\n",
      " [ 59.76855408  49.76855408]\n",
      " [120.10939479  35.72576662]\n",
      " [ 60.60686518  50.60686518]\n",
      " [119.75393751  34.1567534 ]\n",
      " [118.99345743  36.13987856]\n",
      " [118.52108843  35.18335992]\n",
      " [120.71095997  35.44426331]\n",
      " [ 59.30714242  49.30714242]\n",
      " [119.31997528  35.2322537 ]\n",
      " [ 59.4138342   49.4138342 ]\n",
      " [121.17944012  34.53082435]\n",
      " [119.97347874  34.11812535]\n",
      " [ 59.31643068  49.31643068]\n",
      " [118.16379463  35.50799133]\n",
      " [120.43693817  36.19064627]\n",
      " [120.24380071  34.43592137]\n",
      " [ 60.96273257  50.96273257]\n",
      " [119.46974238  34.20712717]\n",
      " [ 58.93979009  48.93979009]\n",
      " [119.99202736  36.47994414]\n",
      " [ 61.24571738  51.24571738]\n",
      " [121.04955272  34.46476479]\n",
      " [ 57.05090557  47.05090557]\n",
      " [ 61.57699648  51.57699648]\n",
      " [119.44777696  35.63293182]\n",
      " [118.77045041  35.49669922]\n",
      " [120.36163603  34.35488025]\n",
      " [ 58.8991786   48.8991786 ]\n",
      " [118.67181395  35.19686124]\n",
      " [117.8481846   34.28084668]\n",
      " [120.75892859  35.28119142]\n",
      " [119.75130887  36.60734558]\n",
      " [ 58.99817515  48.99817515]\n",
      " [ 58.62350355  48.62350355]\n",
      " [ 59.77566034  49.77566034]\n",
      " [ 58.94006362  48.94006362]\n",
      " [118.37245756  35.04808495]\n",
      " [ 60.49115375  50.49115375]\n",
      " [119.36226002  34.46900304]\n",
      " [121.30434024  33.33750801]\n",
      " [ 60.22320225  50.22320225]\n",
      " [ 59.30714424  49.30714424]\n",
      " [ 60.72399033  50.72399033]\n",
      " [120.51360011  34.46729916]\n",
      " [ 60.45612083  50.45612083]\n",
      " [119.06066461  34.85591244]\n",
      " [ 58.95268229  48.95268229]\n",
      " [119.3884822   33.5933389 ]\n",
      " [ 60.47764558  50.47764558]\n",
      " [ 60.54668008  50.54668008]\n",
      " [120.6725737   36.89988193]\n",
      " [ 60.14037506  50.14037506]\n",
      " [ 58.03741281  48.03741281]\n",
      " [ 59.2675079   49.2675079 ]\n",
      " [ 60.12951725  50.12951725]\n",
      " [ 58.82027479  48.82027479]\n",
      " [ 61.3957892   51.3957892 ]\n",
      " [118.98417818  35.06167985]\n",
      " [121.10818282  33.96009407]\n",
      " [118.59248831  34.22218331]\n",
      " [ 58.2504161   48.2504161 ]\n",
      " [119.23674084  33.1951179 ]\n",
      " [119.52616065  34.98554773]\n",
      " [ 60.53580146  50.53580146]\n",
      " [120.34361829  33.23695984]\n",
      " [ 60.78598603  50.78598603]\n",
      " [ 59.89410555  49.89410555]\n",
      " [119.3585184   35.43192254]\n",
      " [120.33849641  34.58471209]\n",
      " [118.39244014  34.23727522]\n",
      " [120.08182936  34.90111035]\n",
      " [ 59.62789453  49.62789453]\n",
      " [119.945106    35.28555407]\n",
      " [119.02834327  33.62038184]\n",
      " [121.5033983   35.87736229]\n",
      " [118.40700627  35.44047474]\n",
      " [120.04886007  35.04059169]\n",
      " [120.54146273  35.75915516]\n",
      " [ 61.72580673  51.72580673]\n",
      " [121.05605681  35.22323891]\n",
      " [121.61222063  35.89683932]\n",
      " [ 59.98750068  49.98750068]\n",
      " [ 57.37920691  47.37920691]\n",
      " [ 60.89534632  50.89534632]\n",
      " [120.45588777  37.16500234]\n",
      " [121.07600714  35.02131165]\n",
      " [120.23378591  33.44410435]\n",
      " [119.11614256  35.15372511]\n",
      " [ 60.05312065  50.05312065]\n",
      " [ 59.83951038  49.83951038]\n",
      " [ 59.95648533  49.95648533]\n",
      " [ 59.67325524  49.67325524]\n",
      " [ 58.87564664  48.87564664]\n",
      " [ 59.23179278  49.23179278]\n",
      " [ 62.74750484  52.74750484]\n",
      " [118.84900642  35.37569802]\n",
      " [119.20747926  34.88526356]\n",
      " [118.44933657  35.06856297]\n",
      " [ 60.1393972   50.1393972 ]\n",
      " [ 61.09736832  51.09736832]\n",
      " [ 59.17490174  49.17490174]\n",
      " [ 60.52456701  50.52456701]\n",
      " [119.37623104  36.91403135]\n",
      " [119.81710356  36.37487642]\n",
      " [120.89959988  35.30729952]\n",
      " [119.09629814  35.32435928]\n",
      " [ 60.50178419  50.50178419]\n",
      " [ 59.66494197  49.66494197]\n",
      " [ 60.99581539  50.99581539]\n",
      " [ 59.91617337  49.91617337]\n",
      " [122.07408267  34.08061541]\n",
      " [ 59.40120561  49.40120561]\n",
      " [119.89555078  34.83117828]\n",
      " [121.02406253  35.59252695]\n",
      " [121.62861555  33.61989854]\n",
      " [121.26570784  34.13382515]\n",
      " [ 60.10325476  50.10325476]\n",
      " [ 60.45784839  50.45784839]\n",
      " [ 60.87832308  50.87832308]\n",
      " [ 61.11954613  51.11954613]\n",
      " [118.70531852  36.16082679]\n",
      " [121.17312464  35.36964219]\n",
      " [ 60.23040071  50.23040071]\n",
      " [ 61.49652926  51.49652926]\n",
      " [ 59.429387    49.429387  ]\n",
      " [ 60.1314726   50.1314726 ]\n",
      " [119.54993453  35.62284993]\n",
      " [120.35701549  34.3070904 ]\n",
      " [120.02451017  35.49799829]\n",
      " [120.59065483  36.10870358]\n",
      " [119.29565631  33.5915387 ]\n",
      " [119.90141187  35.01884962]\n",
      " [118.98789562  33.34514333]\n",
      " [ 59.47774936  49.47774936]\n",
      " [ 60.11066537  50.11066537]\n",
      " [ 60.52466241  50.52466241]\n",
      " [119.01037186  35.94077119]\n",
      " [120.21409374  33.75426122]\n",
      " [ 60.97337884  50.97337884]\n",
      " [120.55832691  35.07600539]\n",
      " [119.48271155  36.40934744]\n",
      " [ 57.8752286   47.8752286 ]\n",
      " [ 59.73743867  49.73743867]\n",
      " [118.59368254  34.91689443]\n",
      " [120.58392819  34.64070791]\n",
      " [ 58.97054252  48.97054252]\n",
      " [118.59739473  36.74957674]\n",
      " [ 59.91410699  49.91410699]\n",
      " [121.64496771  34.75096396]\n",
      " [121.31030875  36.39568381]] [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]] [[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "Forward Propagation Output (y) and NN Output: [[0.83056853]] [[0.83056853]]\n",
      "Epoch 0, Train Loss: 0.028707023414130534, Validation Loss: 1.8049576649435642e-08\n",
      "Manually Updated H2: [[1.85946238]\n",
      " [1.15647869]\n",
      " [0.85946238]\n",
      " [1.65647869]]\n",
      "Manually Updated B2: [[1.05100347]]\n",
      "Updated H2 from Network: [[1.85946238]\n",
      " [1.15647869]\n",
      " [0.85946238]\n",
      " [1.65647869]]\n",
      "Updated B2 from Network: [[1.05100347]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nnet import Network, FCLayer, ActivationLayer\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The function sigtobinary is sigmoid to binary conversion function\n",
    "def sigtobinary(output):\n",
    "    n = (output > 0.5).astype(int)\n",
    "    return n\n",
    "\n",
    "# The function testingFCLayer is testing the forward and backward propagation function\n",
    "def testingFCLayer():\n",
    "    # Initializing the layer and manually assigning Bias and Weights to the layer\n",
    "    layer = FCLayer(2, 3)\n",
    "    layer.weights = np.array([[1, 0, 1], [0, 1, 0]])\n",
    "    layer.biases = np.array([[1, 0, 0]])\n",
    "    # Input data and output of the forward propagation\n",
    "    X = np.array([[1, 1]])\n",
    "    output = layer.forwardprop(X)\n",
    "    print(\"Forward Output:\")\n",
    "    print(output)\n",
    "    # Now I am taking a random output error and testing the backward propagation whether it is working or not\n",
    "    operr = np.array([[0.2, -0.1, 0.05]])\n",
    "    layer.backwardprop(operr, lrate=0.01)\n",
    "    print(\"Updated Weights after Backpropagation:\")\n",
    "    print(layer.weights)\n",
    "    print(\"Updated Biases after Backpropagation:\")\n",
    "    print(layer.biases)\n",
    "\n",
    "# This function is for testing the ActivationLayer\n",
    "def testingActivationlayer():\n",
    "    # Here I am taking sigmoid activation function and initializing the layer\n",
    "    actLayer = ActivationLayer(\"sigmoid\")\n",
    "    # The below is the input data\n",
    "    X = np.array([[0.5, -0.5], [1.5, 2.5]])\n",
    "    # Output of the forward propagation\n",
    "    output = actLayer.forwardprop(X)\n",
    "    print(\"Sigmoid Activation Forward Output:\")\n",
    "    print(output)\n",
    "    # Taking output error and checking the back propagation\n",
    "    operr = np.array([[0.05, -0.02], [0.1, 0.05]])\n",
    "    bkop = actLayer.backwardprop(operr, lrate=0.01)\n",
    "    print(bkop)\n",
    "    print(\"Backpropagation completed. Gradients applied.\")\n",
    "\n",
    "# This function is used to test the network on XOR data\n",
    "def testingXOR():\n",
    "    # So first I am creating a dataset for training and validation\n",
    "    Xtrain = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    Ytrain = np.array([[0], [1], [1], [0]])\n",
    "    Xval = np.array([[0 , 1], [1, 0], [1, 1]])\n",
    "    Yval = np.array([[1], [1], [0]])\n",
    "    # Now I am creating a network with one hidden layer and output layer with tanh and sigmoid activations respectively\n",
    "    XORnet = Network()\n",
    "    XORnet.add(FCLayer(2, 4))\n",
    "    XORnet.add(ActivationLayer(\"tanh\"))\n",
    "    XORnet.add(FCLayer(4, 1))\n",
    "    XORnet.add(ActivationLayer(\"sigmoid\"))\n",
    "    print(\"Training on XOR Function:\")\n",
    "    # Training and validating the network and plotting the loss on the graph\n",
    "    tlossHist, vlossHist = XORnet.fit(Xtrain, Ytrain, Xval, Yval, epochs=5000, lrate=0.1)\n",
    "    plt.plot(tlossHist, label=\"Train Loss\")\n",
    "    plt.plot(vlossHist, label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Stress Classification Loss Curve\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    print(\"Predictions for XOR:\")\n",
    "    # Predicting the output using the trained neural network \n",
    "    predXOR = XORnet.predict(Xval)\n",
    "    predXORbinary = sigtobinary(predXOR)\n",
    "    return Xval, Yval, predXORbinary\n",
    "\n",
    "# I am creating the data for stress and not stress in the below function\n",
    "def generateData(samples=100):\n",
    "    # First I wrote the mean and covariances\n",
    "    # And I used multivariate_normal function to create a distribution for stress (Xstress) and not stress (Xnstress) individually\n",
    "    stressm = np.array([120, 35])\n",
    "    stresscov = np.array([[1, 0], [0, 1]])\n",
    "    Xstress = np.random.multivariate_normal(stressm, stresscov, samples)\n",
    "    Ystress = np.ones((samples, 1))\n",
    "    nstressMean = np.array([60, 50])\n",
    "    nstressCov = np.array([[1, 1], [1, 1]])\n",
    "    Xnstress = np.random.multivariate_normal(nstressMean, nstressCov, samples)\n",
    "    Ynstress = np.zeros((samples, 1))\n",
    "    # Now I stack the stress and not stress values of both X and Y\n",
    "    Xall = np.vstack((Xstress, Xnstress))\n",
    "    Yall = np.vstack((Ystress, Ynstress))\n",
    "    indices = np.arange(Xall.shape[0])\n",
    "    # Then I shuffle the data randomly for better performance (avoids bias and ensures generalization)\n",
    "    np.random.shuffle(indices)\n",
    "    # Then I split the data into train and validation\n",
    "    split = int(0.9 * len(indices))\n",
    "    return Xall[indices][:split], Yall[indices][:split], Xall[indices][split:], Yall[indices][split:]\n",
    "\n",
    "# Using the data created from the above, I test for classifying the stress\n",
    "def testingStress():\n",
    "    Xtrain, Ytrain, Xval, Yval = generateData(samples=1000)\n",
    "    # Creating the neural netwrok for stress (It has hidden layer and output layer with tanh and sigmoid)\n",
    "    stressNet = Network()\n",
    "    stressNet.add(FCLayer(2, 5))\n",
    "    stressNet.add(ActivationLayer(\"tanh\"))\n",
    "    stressNet.add(FCLayer(5, 1))\n",
    "    stressNet.add(ActivationLayer(\"sigmoid\"))\n",
    "    print(\"\\nTraining on Stress Classification:\")\n",
    "    # Now I fit the neural network and get the loss for train and validation and plot them in graph\n",
    "    tloss, vloss = stressNet.fit(Xtrain, Ytrain, Xval, Yval, epochs=5000, lrate=0.1, loss=\"mse\")\n",
    "    plt.plot(tloss, label=\"Train Loss\")\n",
    "    plt.plot(vloss, label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Stress Classification Loss Curve\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    # Using predict function I test the neural network\n",
    "    print(\"Predictions for Stress Classification:\")\n",
    "    predstress = stressNet.predict(Xval)\n",
    "    predstressbin = sigtobinary(predstress)\n",
    "    return Xval, Yval, predstressbin\n",
    "\n",
    "# Calling the functions here\n",
    "testingFCLayer()\n",
    "testingActivationlayer()\n",
    "Xtrain, ytrain, predXORbinary = testingXOR()\n",
    "Xstress, Ystress, predstressbin = testingStress()\n",
    "\n",
    "print(\"Test Data for XOR Network:\")\n",
    "print(Xtrain, ytrain, predXORbinary)\n",
    "\n",
    "print(\"\\nTest Data for Stress Classification:\")\n",
    "print(Xstress, Ystress, predstressbin)\n",
    "\n",
    "# Now this function is for testing the neural network with the data from the first question\n",
    "def testingNN():\n",
    "    X = np.array([[1, 0]])\n",
    "    Xval = np.array([[0, 1]])\n",
    "    H1 = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
    "    B1 = np.array([[0.15, 0.15, 0.15, 0.15]])\n",
    "    H2 = np.array([[1], [1], [0], [1.5]])\n",
    "    B2 = np.array([[0]])\n",
    "    target = 1 \n",
    "    lr = 10\n",
    "    # Creating the neural network with tanh activations for both hidden layer and output layer\n",
    "    nn = Network()\n",
    "    nn.add(FCLayer(2, 4))\n",
    "    nn.add(ActivationLayer(\"tanh\"))\n",
    "    nn.add(FCLayer(4, 1))\n",
    "    nn.add(ActivationLayer(\"tanh\"))\n",
    "    # Here I am manually setting the weights and Bias for the network\n",
    "    nn.layers[0].weights = H1\n",
    "    nn.layers[0].biases = B1\n",
    "    nn.layers[2].weights = H2\n",
    "    nn.layers[2].biases = B2\n",
    "    # Computing the output using forward propagation\n",
    "    A1 = np.dot(X, H1) + B1\n",
    "    h = tanh(A1)\n",
    "    A2 = h.dot(H2) + B2\n",
    "    manualy = tanh(A2)\n",
    "    output = nn.predict(X)\n",
    "    print(\"Forward Propagation Output (y) and NN Output:\", manualy, output)\n",
    "    Y = np.array([[1]])\n",
    "    Yval = np.array([[1]])\n",
    "    alpha = 10\n",
    "    # Using backward propagation computing the new H2 and B2\n",
    "    dEdy = 2 * (manualy - Y)\n",
    "    dydA2 = tanDer(A2)\n",
    "    dEdA2 = dEdy * dydA2\n",
    "    dEdH2 = h.T.dot(dEdA2)\n",
    "    dEdB2 = dEdA2\n",
    "    H2new = H2 - alpha * dEdH2\n",
    "    B2new = B2 - alpha * dEdB2\n",
    "    nn.fit(X, Y, Xval, Yval, epochs=1, lrate=alpha, loss=\"mse\") # I used the MSE loss function\n",
    "    H2updated = nn.layers[2].weights\n",
    "    B2updated = nn.layers[2].biases\n",
    "    print(\"Manually Updated H2:\", H2new)\n",
    "    print(\"Manually Updated B2:\", B2new)\n",
    "    print(\"Updated H2 from Network:\", H2updated)\n",
    "    print(\"Updated B2 from Network:\", B2updated)\n",
    "    return nn\n",
    "\n",
    "nn_test = testingNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cae489-fa3e-46d0-b29b-227a5e2f61c9",
   "metadata": {},
   "source": [
    "# Manual Calculation for the individual testing function\n",
    "\n",
    "## 1. Forward Propagation for FCLayer\n",
    "\n",
    "Given:\n",
    "\n",
    "$\n",
    "W = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}, \\quad X = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "The forward propagation formula is:\n",
    "\n",
    "$\n",
    "Z = XW + B\n",
    "$\n",
    "\n",
    "Now, performing the matrix multiplication and addition:\n",
    "\n",
    "$\n",
    "Z = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\times \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} + \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "Z = \\begin{bmatrix} (1 \\times 1 + 1 \\times 0) & (1 \\times 0 + 1 \\times 1) & (1 \\times 1 + 1 \\times 0) \\end{bmatrix} + \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "Z = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix} + \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "Z = \\begin{bmatrix} 2 & 1 & 1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Forward Output:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} 2 & 1 & 1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "## 2. Backward Propagation for FCLayer\n",
    "\n",
    "Given:\n",
    "\n",
    "$\n",
    "E = \\begin{bmatrix} 0.2 & -0.1 & 0.05 \\end{bmatrix}, \\quad \\eta = 0.01\n",
    "$\n",
    "\n",
    "Computing Gradient,\n",
    "\n",
    "Weight Gradient:\n",
    "\n",
    "$\n",
    "\\Delta W = X^T \\times E\n",
    "$\n",
    "\n",
    "$\n",
    "\\Delta W = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\times \\begin{bmatrix} 0.2 & -0.1 & 0.05 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\Delta W = \\begin{bmatrix} 0.2 & -0.1 & 0.05 \\\\ 0.2 & -0.1 & 0.05 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Bias Gradient:\n",
    "\n",
    "$\n",
    "\\Delta B = E\n",
    "$\n",
    "\n",
    "Updating weights:\n",
    "\n",
    "$\n",
    "W' = W - \\eta \\Delta W\n",
    "$\n",
    "\n",
    "$\n",
    "W' = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix} - 0.01 \\times \\begin{bmatrix} 0.2 & -0.1 & 0.05 \\\\ 0.2 & -0.1 & 0.05 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "W' = \\begin{bmatrix} 1 - 0.002 & 0 + 0.001 & 1 - 0.0005 \\\\ 0 - 0.002 & 1 + 0.001 & 0 - 0.0005 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "W' = \\begin{bmatrix} 0.998 & 0.001 & 0.9995 \\\\ -0.002 & 1.001 & -0.0005 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Updating Bias,\n",
    "\n",
    "$\n",
    "B' = B - \\eta \\Delta B\n",
    "$\n",
    "\n",
    "$\n",
    "B' = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} - 0.01 \\times \\begin{bmatrix} 0.2 & -0.1 & 0.05 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "B' = \\begin{bmatrix} 1 - 0.002 & 0 + 0.001 & 0 - 0.0005 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "B' = \\begin{bmatrix} 0.998 & 0.001 & -0.0005 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "## 3. Forward Propagation for Activation Layer\n",
    "\n",
    "Given:\n",
    "\n",
    "$\n",
    "X = \\begin{bmatrix} 0.5 & -0.5 \\\\ 1.5 & 2.5 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "The sigmoid function is defined as:\n",
    "\n",
    "$\n",
    "S(x) = \\frac{1}{1 + e^{-x}}\n",
    "$\n",
    "\n",
    "Now, applying the sigmoid function element-wise:\n",
    "\n",
    "$\n",
    "S(0.5) = \\frac{1}{1 + e^{-0.5}} \\approx 0.622\n",
    "$\n",
    "\n",
    "$\n",
    "S(-0.5) = \\frac{1}{1 + e^{0.5}} \\approx 0.378\n",
    "$\n",
    "\n",
    "$\n",
    "S(1.5) = \\frac{1}{1 + e^{-1.5}} \\approx 0.818\n",
    "$\n",
    "\n",
    "$\n",
    "S(2.5) = \\frac{1}{1 + e^{-2.5}} \\approx 0.924\n",
    "$\n",
    "\n",
    "$\n",
    "S(X) = \\begin{bmatrix} 0.622 & 0.378 \\\\ 0.818 & 0.924 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Forward Output:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} 0.622 & 0.378 \\\\ 0.818 & 0.924 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "## 4. Backward Propagation for Activation Layer\n",
    "\n",
    "Given:\n",
    "\n",
    "$\n",
    "E = \\begin{bmatrix} 0.05 & -0.02 \\\\ 0.1 & 0.05 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "The derivative of the sigmoid function is:\n",
    "\n",
    "$\n",
    "S'(x) = S(x)(1 - S(x))\n",
    "$\n",
    "\n",
    "Now, calculating the sigmoid derivative element-wise:\n",
    "\n",
    "$\n",
    "S'(0.5) = 0.622(1 - 0.622) \\approx 0.235\n",
    "$\n",
    "\n",
    "$\n",
    "S'(-0.5) = 0.378(1 - 0.378) \\approx 0.235\n",
    "$\n",
    "\n",
    "$\n",
    "S'(1.5) = 0.818(1 - 0.818) \\approx 0.149\n",
    "$\n",
    "\n",
    "$\n",
    "S'(2.5) = 0.924(1 - 0.924) \\approx 0.070\n",
    "$\n",
    "\n",
    "$\n",
    "S'(X) = \\begin{bmatrix} 0.235 & 0.235 \\\\ 0.149 & 0.070 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "Now, backpropagating the error:\n",
    "\n",
    "$\n",
    "E' = E \\times S'(X)\n",
    "$\n",
    "\n",
    "$\n",
    "E' = \\begin{bmatrix} 0.05 \\times 0.235 & -0.02 \\times 0.235 \\\\ 0.1 \\times 0.149 & 0.05 \\times 0.070 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "E' = \\begin{bmatrix} 0.0118 & -0.0047 \\\\ 0.0149 & 0.0035 \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f55996-b265-402b-9ea9-c01b9f78a121",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "### (a) Explain how the learning rate impacts the gradient descent algorithm\n",
    "\n",
    "In gradient descent the learning rate is a hyperparameter that has significant role in how quickly the algorithm will converge to the minimum of the loss function. The parameters of the model are updated iteratively by the following equation\n",
    "\n",
    "$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla J(\\theta^{(t)})\n",
    "$\n",
    "\n",
    "In this formula $ \\theta^{(t)} $ represents the model parameters at iteration $ t $ with $ \\alpha $ as the learning rate and $ \\nabla J(\\theta^{(t)}) $ is the gradient of the loss function $ J(\\theta) $ with respect to $ \\theta $ and $ \\theta^{(t+1)} $ is the updated value of the parameters for the next iteration. The learning rate $ \\alpha $ determines the size of the step the algorithm takes in the direction of the negative gradient therefore controlling how fast the model parameters are adjusted.\n",
    "\n",
    "A small learning rate implies the parameter updates are small and incremental and provides stable and smooth convergence to the optimal solution. But it can also make the learning process very slow requiring many iterations to converge to the minimum. On the other hand a high learning rate allows updates to be done more quickly, speeding up convergence. But if the learning rate is too high, the algorithm will overshoot the optimum point and oscillate around the minimum, or diverge entirely. This results in poor performance and instability in the learning process.\n",
    "\n",
    "Therefore the optimal learning rate is necessary for effective training. A low learning rate would make the optimization process excessively time consuming and a learning rate that is too high might lead to the algorithm failing to converge. It is very important to find a balance between these two extremes in order to achieve effective and stable learning without running into issues like slow convergence or divergence.\n",
    "\n",
    "### (b) Detail how AdaGrad works and why or how RMSprop improves on AdaGrad\n",
    "\n",
    "AdaGrad (Adaptive Gradient Algorithm) is an optimizing algorithm that adapts the learning rate for each parameter individually based on the accumulation of past squared gradients. It is effective in sparse feature setting because it allows for infrequently updated parameters to get larger updates and reduce the learning rate for frequently updated parameters. This encourages models to converge faster if a few features are more informative than others. The parameter update rule in AdaGrad is given by\n",
    "\n",
    "$\n",
    "\\theta_i = \\theta_i - \\frac{\\eta}{\\sqrt{G_{ii} + \\epsilon}} \\cdot \\nabla_\\theta J(\\theta)\n",
    "$\n",
    "\n",
    "where $ G_{ii} $ represents the sum of squared gradients for parameter $ i $ and $ \\eta $ is the learning rate and $ \\epsilon $ is a small constant to prevent division by zero. The significance is that parameters with bigger cumulative gradients receive smaller updates, while the remaining parameters with smaller gradients have comparatively higher learning rate.\n",
    "\n",
    "The sequential working of AdaGrad in a model can be described as follows: \n",
    "\n",
    "1. Initialize parameters $ \\theta $ and set the sum of squared gradients $ G = 0 $ for all parameters.\n",
    "2. Compute gradients $ \\nabla_\\theta J(\\theta) $ for the current batch of data.  \n",
    "3. Accumulate squared gradients for each parameter  \n",
    "\n",
    "   $\n",
    "   G_{ii} = G_{ii} + \\nabla_\\theta J(\\theta)^2\n",
    "   $\n",
    "\n",
    "4. Adjust the learning rate by scaling it inversely with $ \\sqrt{G_{ii}} $ which ensure that frequently updated parameters have smaller step sizes.  \n",
    "5. Update parameters using the modified learning rate.  \n",
    "6. Repeat the process for each batch until convergence.  \n",
    "\n",
    "Although AdaGrad possesses a sparsity handling advantage and ensures smooth convergence, it also possesses one major disadvantage. Since $ G_{ii} $ continuously accumulates squared gradients, it keeps growing over time. That makes the learning rate reduce continuously over time. This can shrink updates to excessively small values delaying convergence or even stopping learning.\n",
    "\n",
    "To prevent AdaGrad's vanishing learning rate issue, RMSprop (Root Mean Squared Propagation) changes the way gradient information is accumulated. Instead of adding up all past squared gradients RMSprop uses an exponentially weighted moving average. This makes more recent gradients stronger and older ones fade away, so the learning rate does not become excessively small.  \n",
    "\n",
    "RMSprop maintains an exponentially weighted average of past squared gradients:  \n",
    "\n",
    "$\n",
    "v_i = \\beta v_{i-1} + (1 - \\beta) \\nabla_\\theta J(\\theta)^2\n",
    "$\n",
    "\n",
    "where $ v_i $ represents the moving average of squared gradients, and $ \\beta $ (typically set to 0.9) controls how much past gradients contribute. This ensures that the denominator stabilizes over time preventing the learning rate from shrinking too aggressively. The parameter update step is then given by\n",
    "\n",
    "$\n",
    "\\theta_i = \\theta_i - \\frac{\\eta}{\\sqrt{v_i + \\epsilon}} \\cdot \\nabla_\\theta J(\\theta)\n",
    "$\n",
    "\n",
    "\n",
    "By maintaining the learning rate constant RMSprop allows optimization to proceed efficiently even in deep networks and complex non convex loss functions. It is particularly useful for deep learning, where gradients are of a large range, and it is desirable to prevent extremely small updates for efficient training. In contrast to AdaGrad, which suffers from long-term learning rate decay, RMSprop allows continued learning by adding more weight to recent gradient information while still scaling the learning rate for each parameter.\n",
    "\n",
    "### (c) Detail how the Adam optimizer works\n",
    "\n",
    "Adam optimizer is among the popular optimization algorithms for deep learning. It combines the best of two other optimizers which are Momentum and RMSprop and offer faster convergence as well as improved stability while training. Adam estimates adaptive learning rates for all parameters based on both the first and second moments of the gradients. The first one is the moving average of the gradients or momentum, and the second one is the moving average of the squares of the gradients, or gradient scale.\n",
    "\n",
    "At each iteration $t$ Adam computes the first moment $m_t$ mean of gradients and the second moment $v_t$ uncentered variance of gradients using the following equations\n",
    "\n",
    "$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta J(\\theta)\n",
    "$\n",
    "$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla_\\theta J(\\theta)^2\n",
    "$\n",
    "\n",
    "Where $m_t$ and $v_t$ are the first and second moment estimates, $\\beta_1$ and $\\beta_2$ are decay rates for the moving averages (typically set to $0.9$ and $0.999$, respectively) and $\\nabla_\\theta J(\\theta)$ is the gradient of the loss function with respect to the parameters. These equations iteratively update the first and second moment estimates by incorporating both the previous moments and the current gradient information.\n",
    "\n",
    "However since $m_t$ and $v_t$ are initialized as zero vectors they are biased toward zero especially at the start of training. To overcome this bias Adam employs bias correction as below\n",
    "\n",
    "$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "$\n",
    "$\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$\n",
    "\n",
    "The bias-corrected first and second moments $\\hat{m}_t$ and $\\hat{v}_t$, provide more accurate estimates, especially during the early iterations when the moment estimates are still close to zero.\n",
    "\n",
    "Once the bias corrected moments are computed Adam updates the parameters $\\theta_i$ using the following update rule:\n",
    "\n",
    "$\n",
    "\\theta_i = \\theta_i - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "$\n",
    "\n",
    "Where $\\eta$ is the learning rate and $\\hat{v}_t$ is the bias-corrected second moment estimate, $\\hat{m}_t$ is the bias-corrected first moment estimate and $\\epsilon$ is a small constant added to prevent division by zero.\n",
    "\n",
    "One of the primary advantages of Adam is that it can change the learning rates adaptively for each parameter. With both the first moment (momentum) and second moment (adaptive learning rates), Adam offers more stable and faster convergence. By using bias correction as well, it improves the accuracy of the moment estimates, especially during early training. Also, the adaptive learning rate mechanism stabilizes training, especially in large parameter spaces of complex models.\n",
    "\n",
    "### (d) Explain the difference between Bagging and Boosting Methods (make sure you talk about bias/variance and the relationship to decision trees)\n",
    "\n",
    "Bagging (Bootstrap Aggregating) and Boosting are ensemble learning algorithms designed to be capable of improving the performance of machine learning models, specifically decision trees. Although they both aim to do that, they use different techniques for training base models and combining their predictions, which produces different effects on bias and variance.\n",
    "\n",
    "Bagging separately trains multiple base models (typically decision trees) on multiple bootstrap samples (random subsets of data with replacements) of the training set. As for training the final prediction comes from averaging out the predictions when using regression or majority voting in classification. Bagging's most important benefit is that it does reduce variance and is particularly worth using when training high-variance base models like decision trees. Decision trees lead to overfitting, particularly when they have depth, resulting in high variance. By taking a mean over multiple models learned on different subsets, Bagging smooths these fluctuations, creating a more general and robust model. Bagging does not help in addressing the bias of the model. When the base model is biased (e.g., shallow decision trees that underfit) Bagging will not help to counter this flaw and the model may still underperform.\n",
    "\n",
    "Boosting on the other hand works by sequentially training models with each subsequent model trained to learn from the errors of the preceding ones. In Boosting each model assigns higher weights to the misclassified instances so that the ensemble can focus on the hardest data points. This iterative process reduces both bias and variance. Boosting is most effective when employing weak learners such as shallow decision trees (usually referred to as decision stumps) since it can improve them step by step by concentrating on their mistakes and creating a robust model in the long run. Consequently, Boosting reduces bias by better fitting the data, as well as variance by concentrating on hard cases. However, Boosting is prone to overfitting if too many models are added or if the learning rate is too high since the model will then be too complex.\n",
    "\n",
    "The relationship between decision trees and these methods is significant. Bagging is well with deep decision trees as it reduces the overfitting bias (variance), but does not address bias, i.e., it can still fail if the trees are shallow. Boosting is preferable when weak decision trees are used as base models because it progressively improves them to reduce both bias and variance, leading to better generalization even with shallow base models.\n",
    "\n",
    "### (e) What is Gradient Boosting and describe how it works\n",
    "\n",
    "Gradient Boosting is an advanced boosting algorithm that builds a strong model via the combination of several weak models sequentially. In comparison with other traditional majority voting or weighted averaging, Gradient Boosting uses the gradient of the loss function to guide the training of each subsequent model. The underlying idea is to decrease the residual errors directly by fitting new models to the negative gradient of the loss function to predictions of the present ensemble.\n",
    "\n",
    "The algorithm begins with a simple initial model, in the majority of cases a constant prediction. Next, at each iteration $t$, a new weak model $h_t(x)$ (typically a shallow decision tree) is trained to predict the negative gradient of the loss function at the predictions of the current ensemble. The model is added to the current ensemble, and the predictions are adjusted by considering this new model. The mathematical formula for model update at iteration $t$ is:\n",
    "\n",
    "$\n",
    "F_t(x) = F_{t-1}(x) + \\eta \\cdot h_t(x)\n",
    "$\n",
    "\n",
    "Here, $F_{t-1}(x)$ represents the previous ensemble's prediction, $h_t(x)$ represents the new weak learner and  $\\eta$ represents the learning rate used to controls the magnitude of the update. The learning rate keeps the model from overfitting because it does not update too large correction at each step.\n",
    "\n",
    "In each iteration the gradient of the loss function is computed based on the existing prediction, and the model learns to predict the gradient. In this way, the next model focuses on the residual errors, improving the model's accuracy and performance. Models are built in an ensemble style by iteratively stacking these weak learners, thus making Gradient Boosting a very powerful algorithm for real-world tasks for most of the machine learning problems.\n",
    "\n",
    "The greatest advantage of Gradient Boosting is that it can minimize the loss function directly by learning from the errors of previous models. This enables the model to learn patterns from the data and achieve high prediction accuracy. XGBoost and LightGBM are two algorithms based on this idea, which are used quite heavily in machine learning competitions and even real-world applications. These techniques enhance Gradient Boosting using optimizations such as regularization, parallelization and efficient tree-growing methods to further boost its scalability and efficiency.\n",
    "\n",
    "Steps involved:\n",
    "\n",
    "- Start with an initial model often the mean of the target values in regression, as the base prediction.\n",
    "\n",
    "- Iterate through boosting rounds for each iteration $$:\n",
    "\n",
    "- Calculate the negative gradient (residuals) of the loss function with respect to the current models predictions.\n",
    "\n",
    "- Fit a new weak model (usually a decision tree) to predict the residuals.\n",
    "\n",
    "- Add the new weak models predictions to the current model, scaled by the learning rate $$.\n",
    "\n",
    "- Repeat the process for a set number of iterations $$, progressively improving the model.\n",
    "\n",
    "- The final model is the sum of the predictions from all weak learners.\n",
    "\n",
    "### (f) Explain why neural networks are prone to overfitting\n",
    "\n",
    "Deep Neural networks are extremely flexible models that can learn extremely complicated relationships in the data. This flexibility is why they perform so exceptionally well but are also prone to overfitting. In this situation, the model will not only learn to recognize the underlying patterns in the training data but will also memorize the noise and the random variations. This results in a model that performs well on the training data but fails to generalize to new data and, therefore, performs badly on test data or when put into practice.\n",
    "\n",
    "One of the significant reasons for this overfitting issue is the high capacity of neural networks. Deep networks, with numerous layers and an enormous number of parameters (weights and biases), can memorize the whole training dataset. Such flexibility is a double-edged sword. While it enables the network to learn patterns, it also enables the model to fit noise, especially if the training data set is noisy or small. The lack of regularization is also one of the reasons. Regularization techniques such as dropout L2 regularization or early stopping try to restrict the model's capacity to overfit by penalizing too complicated models or forcing the network to learn more generalizable features. Without these regularization techniques, neural networks have no constraint and can memorize the training data, leading to overfitting. \n",
    "\n",
    "Overfitting also tends to happen when there is a lack of training data. Neural networks work well on large data, and the more data, the better the model can generalize the learned patterns. If the dataset is small, the model will tend to learn false patterns or correlations that do not hold for new unseen data. Cross-validation, regularization, and increasing the amount of training data are usually employed to prevent overfitting. Additionally, methods like early stopping (to halt training when performance on the validation set starts to deteriorate) and dropout (which randomly deactivates a fraction of the neurons during training to discourage reliance on any single feature) can discourage the network from memorizing noise and thus improve its ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cab296-133d-4d77-9956-0dc9cf549813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
